{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9224d4e2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.032505,
     "end_time": "2022-06-04T05:01:27.361680",
     "exception": false,
     "start_time": "2022-06-04T05:01:27.329175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning baseline in Python with stable-baselines3\n",
    "This code will make your life easier if you want to try Reinforcement Learning (RL) as a solution to kaggle's kore 2022 challenge.\n",
    "One of the (multiple) difficulties of RL is achieving a clean implementation. While you can of course try to build yourself\n",
    "one of the RL models described in literature, chances are that you will spend more time debugging your model than actually competing.\n",
    "\n",
    "[stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/#) is powerful RL library with a number of very nice features for this competition:\n",
    "- It implements the most popular modern deep RL algorithms\n",
    "- It is simple and ellegant to use\n",
    "- It is rather well documented\n",
    "- There are plenty of tutorials and examples\n",
    "\n",
    "In other words, it's a fantastic starting point. Alas, it requires an environment compatible with OpenAI-gym and the kore environment is not. What you'll find in this notebook is **KoreGymEnv**, a wrapper around the kore env that makes it play nice with stable-baselines3. It includes very simple feature and action engineering, so the only thing you need to care about is building upon them, choosing a model and throwing yourself into the cold, unforgiving and yet very rewarding reinforcement learning waters ;)\n",
    "\n",
    "As a bonus, this notebook also demonstrates the end-to-end process that you need to follow to submit any model with external dependencies. Click on submit and you're good to go.\n",
    "\n",
    "#### Notes:\n",
    "\n",
    "- In stable-baselines3, states and actions are numpy arrays. In the kore environment, states are lists of dicts and actions are dicts with shipyard ids as keys and shipyard actions as values. Thus, we need an interface to \"translate\" them. This interface is effectively where you implement your state & action engineering. You'll find more details in the KoreGymEnv class.\n",
    "- In the ideal case, you would use self-play and let your agent play a very large number of games against itself, improving at ever step. Unfortunately, [it is not clear how to implement self-play in the kore env](https://www.kaggle.com/competitions/kore-2022/discussion/323382). So we have to train against static opponents. In this baseline, we'll use the starter bot. Of course, nothing prevents you from implementing pseudo-self-play and train against ever improving versions of your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8caa1",
   "metadata": {
    "papermill": {
     "duration": 0.029652,
     "end_time": "2022-06-04T05:01:27.421167",
     "exception": false,
     "start_time": "2022-06-04T05:01:27.391515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## tl; dr\n",
    "\n",
    "```python\n",
    "# Train a PPO agent\n",
    "from environment import KoreGymEnv\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "kore_env = KoreGymEnv()\n",
    "model = PPO('MlpPolicy', kore_env, verbose=1)\n",
    "model.learn(total_timesteps=100000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30927d53",
   "metadata": {
    "papermill": {
     "duration": 0.029408,
     "end_time": "2022-06-04T05:01:27.480193",
     "exception": false,
     "start_time": "2022-06-04T05:01:27.450785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31adf55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:01:27.542838Z",
     "iopub.status.busy": "2022-06-04T05:01:27.542332Z",
     "iopub.status.idle": "2022-06-04T05:01:58.848808Z",
     "shell.execute_reply": "2022-06-04T05:01:58.847737Z"
    },
    "papermill": {
     "duration": 31.341751,
     "end_time": "2022-06-04T05:01:58.851782",
     "exception": false,
     "start_time": "2022-06-04T05:01:27.510031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3\r\n",
      "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.7/177.7 KB\u001b[0m \u001b[31m414.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting gym\r\n",
      "  Downloading gym-0.24.0.tar.gz (694 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.4/694.4 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: gym\r\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.24.0-py3-none-any.whl size=790684 sha256=d3d90ba54aef0401e2e575a3733dd608c9c4f4b8ddfb195fd18dce406b4ca5be\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/7a/27/ac44ac6e4763637d0ed8d89f686465960f9837440ee880abe6\r\n",
      "Successfully built gym\r\n",
      "Installing collected packages: stable-baselines3, gym\r\n",
      "Successfully installed gym-0.24.0 stable-baselines3-1.5.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --target=lib --no-deps stable-baselines3 gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d4cc4",
   "metadata": {
    "papermill": {
     "duration": 0.048055,
     "end_time": "2022-06-04T05:01:58.949125",
     "exception": false,
     "start_time": "2022-06-04T05:01:58.901070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### A note on dependencies\n",
    "The kaggle notebook environment and the actual competition environment are different. I couldn't find any documentation on the differences other than through comments from more experienced kagglers. So let's take a minute to understand the cell below. I hope that this information saves fellow competitors a lot of time and trial-and-error!\n",
    "\n",
    "`stable-baselines` is not (yet) a part of the kaggle docker environment, so we have to install it manually. In the notebook environment, you start at `/kaggle/working/`, so the cell above installs the libraries into `/kaggle/working/lib/`. We have two options to load the library now, `import lib.stable-baselines3` or add `/kaggle/working/lib/` to [sys.path](https://docs.python.org/3/library/sys.html#sys.path), which tells Python where look for modules.\n",
    "\n",
    "When you submit your agent as an archive, however, your code is unzipped to `/kaggle_simulations/agent/`, _but the working directory remains `/kaggle/working/`_. In the competition env, neither of the options above work, because `lib` isn't `/kaggle/working/lib` anymore, it has been unzipped with the rest of your code to `/kaggle_simulations/agent/lib`. Surprise!\n",
    "\n",
    "The code below then checks whether we are in the simulation environment, and adds the right location of the external dependencies to `sys.path`.\n",
    "\n",
    "Additionally, there is a limit on the submission size, that's why we are installing with `--no-deps` to keep the submission size small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f98aaf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:01:59.049648Z",
     "iopub.status.busy": "2022-06-04T05:01:59.049349Z",
     "iopub.status.idle": "2022-06-04T05:01:59.055165Z",
     "shell.execute_reply": "2022-06-04T05:01:59.054343Z"
    },
    "papermill": {
     "duration": 0.05954,
     "end_time": "2022-06-04T05:01:59.057227",
     "exception": false,
     "start_time": "2022-06-04T05:01:58.997687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, os.path.join(os.getcwd(), 'lib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e66d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T06:52:15.41672Z",
     "iopub.status.busy": "2022-05-20T06:52:15.416056Z",
     "iopub.status.idle": "2022-05-20T06:52:15.421073Z",
     "shell.execute_reply": "2022-05-20T06:52:15.420136Z",
     "shell.execute_reply.started": "2022-05-20T06:52:15.416679Z"
    },
    "papermill": {
     "duration": 0.048491,
     "end_time": "2022-06-04T05:01:59.153738",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.105247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde86b4e",
   "metadata": {
    "papermill": {
     "duration": 0.047503,
     "end_time": "2022-06-04T05:01:59.249222",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.201719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a45662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:01:59.347703Z",
     "iopub.status.busy": "2022-06-04T05:01:59.347079Z",
     "iopub.status.idle": "2022-06-04T05:01:59.354115Z",
     "shell.execute_reply": "2022-06-04T05:01:59.353319Z"
    },
    "papermill": {
     "duration": 0.059436,
     "end_time": "2022-06-04T05:01:59.357138",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.297702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "import numpy as np\n",
    "from kaggle_environments import make\n",
    "\n",
    "# Read env specification\n",
    "ENV_SPECIFICATION = make('kore_fleets', debug = True).specification\n",
    "SHIP_COST = ENV_SPECIFICATION.configuration.spawnCost.default\n",
    "SHIPYARD_COST = ENV_SPECIFICATION.configuration.convertCost.default\n",
    "GAME_CONFIG = {\n",
    "    'episodeSteps':  ENV_SPECIFICATION.configuration.episodeSteps.default,  # You might want to start with smaller values\n",
    "    'size': ENV_SPECIFICATION.configuration.size.default,\n",
    "    'maxLogLength': None\n",
    "}\n",
    "\n",
    "# Define your opponent. We'll use the starter bot in the notebook environment for this baseline.\n",
    "OPPONENT = 'opponent.py'\n",
    "GAME_AGENTS = [None, OPPONENT]\n",
    "\n",
    "# Define our parameters\n",
    "N_1D_FEATURES = 3\n",
    "N_2D_FEATURES = 5\n",
    "MAX_FP_LEN = 10\n",
    "\n",
    "ACTION_SIZE = (4,)\n",
    "#\n",
    "OBSERVATION_SIZE = (GAME_CONFIG[\"size\"] * GAME_CONFIG[\"size\"] * (N_2D_FEATURES + MAX_FP_LEN) + N_1D_FEATURES, )\n",
    "DTYPE = np.float64\n",
    "\n",
    "MAX_OBSERVABLE_KORE = 500\n",
    "MAX_OBSERVABLE_SHIPS = 200\n",
    "\n",
    "MIN_SPAWN_LIMIT = 1\n",
    "MAX_SPAWN_LIMIT = 10\n",
    "\n",
    "MAX_ACTION_FLEET_SIZE = 150\n",
    "MAX_KORE_IN_RESERVE = 40000\n",
    "WIN_REWARD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c7cf13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:01:59.459543Z",
     "iopub.status.busy": "2022-06-04T05:01:59.459265Z",
     "iopub.status.idle": "2022-06-04T05:01:59.507035Z",
     "shell.execute_reply": "2022-06-04T05:01:59.506272Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.102642,
     "end_time": "2022-06-04T05:01:59.509926",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.407284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing opponent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile opponent.py\n",
    "from kaggle_environments.envs.kore_fleets.helpers import *\n",
    "\n",
    "import math\n",
    "import random\n",
    "from typing import Dict, List, Union, Optional, Generator, Tuple\n",
    "from collections import defaultdict\n",
    "from kaggle_environments.envs.kore_fleets.helpers import Configuration\n",
    "from kaggle_environments.envs.kore_fleets.helpers import SPAWN_VALUES\n",
    "import numpy as np\n",
    "#%%writefile expantion.py\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "\n",
    "def max_ships_to_spawn(turns_controlled: int) -> int:\n",
    "    for idx, target in enumerate(SPAWN_VALUES):\n",
    "        if turns_controlled < target:\n",
    "            return idx + 1\n",
    "    return len(SPAWN_VALUES) + 1\n",
    "\n",
    "\n",
    "def max_flight_plan_len_for_ship_count(ship_count: int) -> int:\n",
    "    return math.floor(2 * math.log(ship_count)) + 1\n",
    "\n",
    "\n",
    "def min_ship_count_for_flight_plan_len(flight_plan_len: int) -> int:\n",
    "    return math.ceil(math.exp((flight_plan_len - 1) / 2))\n",
    "\n",
    "\n",
    "def collection_rate_for_ship_count(ship_count: int) -> float:\n",
    "    return min(math.log(ship_count) / 20, 0.99)\n",
    "\n",
    "\n",
    "def create_spawn_ships_command(num_ships: int) -> str:\n",
    "    return f\"SPAWN_{num_ships}\"\n",
    "\n",
    "\n",
    "def create_launch_fleet_command(num_ships: int, plan: str) -> str:\n",
    "    return f\"LAUNCH_{num_ships}_{plan}\"\n",
    "\n",
    "\n",
    "class cached_property:\n",
    "    \"\"\"\n",
    "    python 3.9:\n",
    "    >>> from functools import cached_property\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "        self.key = \"__\" + func.__name__\n",
    "\n",
    "    def __get__(self, instance, owner):\n",
    "        try:\n",
    "            return instance.__getattribute__(self.key)\n",
    "        except AttributeError:\n",
    "            value = self.func(instance)\n",
    "            instance.__setattr__(self.key, value)\n",
    "            return value\n",
    "\n",
    "\n",
    "class cached_call:\n",
    "    \"\"\"\n",
    "    may cause a memory leak, be careful\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "        self.key = \"__\" + func.__name__\n",
    "\n",
    "    def __get__(self, instance, owner):\n",
    "        try:\n",
    "            d = instance.__getattribute__(self.key)\n",
    "        except AttributeError:\n",
    "            d = {}\n",
    "            instance.__setattr__(self.key, d)\n",
    "\n",
    "        def func(x):\n",
    "            try:\n",
    "                return d[x]\n",
    "            except KeyError:\n",
    "                value = self.func(instance, x)\n",
    "                d[x] = value\n",
    "                return value\n",
    "\n",
    "        return func\n",
    "\n",
    "\n",
    "class Obj:\n",
    "    def __init__(self, game_id: Union[str, int]):\n",
    "        self._game_id = game_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(id={self._game_id})\"\n",
    "\n",
    "    @property\n",
    "    def game_id(self):\n",
    "        return self._game_id\n",
    "\n",
    "class Action(Obj):\n",
    "    def __init__(self, dx, dy, game_id, command):\n",
    "        super().__init__(game_id)\n",
    "        self._dx = dx\n",
    "        self._dy = dy\n",
    "        self._command = command\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._command\n",
    "\n",
    "    @property\n",
    "    def dx(self) -> int:\n",
    "        return self._dx\n",
    "\n",
    "    @property\n",
    "    def dy(self) -> int:\n",
    "        return self._dy\n",
    "\n",
    "    @property\n",
    "    def command(self) -> str:\n",
    "        return self._command\n",
    "\n",
    "\n",
    "North = Action(\n",
    "    dx=0,\n",
    "    dy=1,\n",
    "    game_id=0,\n",
    "    command=\"N\",\n",
    ")\n",
    "East = Action(\n",
    "    dx=1,\n",
    "    dy=0,\n",
    "    game_id=1,\n",
    "    command=\"E\",\n",
    ")\n",
    "South = Action(\n",
    "    dx=0,\n",
    "    dy=-1,\n",
    "    command=\"S\",\n",
    "    game_id=2,\n",
    ")\n",
    "West = Action(\n",
    "    dx=-1,\n",
    "    dy=0,\n",
    "    command=\"W\",\n",
    "    game_id=3,\n",
    ")\n",
    "Convert = Action(\n",
    "    dx=0,\n",
    "    dy=0,\n",
    "    command=\"C\",\n",
    "    game_id=-1,\n",
    ")\n",
    "\n",
    "\n",
    "ALL_DIRECTIONS = {North, East, South, West}\n",
    "ALL_ACTIONS = {North, East, South, West, Convert}\n",
    "GAME_ID_TO_ACTION = {x.game_id: x for x in ALL_ACTIONS}\n",
    "COMMAND_TO_ACTION = {x.command: x for x in ALL_ACTIONS}\n",
    "ACTION_TO_OPPOSITE_ACTION = {\n",
    "    North: South,\n",
    "    East: West,\n",
    "    South: North,\n",
    "    West: East,\n",
    "}\n",
    "\n",
    "\n",
    "def get_opposite_action(action):\n",
    "    return ACTION_TO_OPPOSITE_ACTION.get(action, action)\n",
    "\n",
    "\n",
    "class Point(Obj):\n",
    "    def __init__(self, x: int, y: int, kore: float, field: \"Field\"):\n",
    "        super().__init__(game_id=(field.size - y - 1) * field.size + x)\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._kore = kore\n",
    "        self._field = field\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Point({self._x}, {self._y})\"\n",
    "\n",
    "    @property\n",
    "    def x(self) -> int:\n",
    "        return self._x\n",
    "\n",
    "    @property\n",
    "    def y(self) -> int:\n",
    "        return self._y\n",
    "\n",
    "    def to_tuple(self) -> Tuple[int, int]:\n",
    "        return self._x, self._y\n",
    "\n",
    "    @property\n",
    "    def kore(self) -> float:\n",
    "        return self._kore\n",
    "\n",
    "    def set_kore(self, kore: float):\n",
    "        self._kore = kore\n",
    "\n",
    "    @property\n",
    "    def field(self) -> \"Field\":\n",
    "        return self._field\n",
    "\n",
    "    def apply(self, action: Action) -> \"Point\":\n",
    "        return self._field[(self.x + action.dx, self.y + action.dy)]\n",
    "\n",
    "    @cached_call\n",
    "    def distance_from(self, point: \"Point\") -> int:\n",
    "        return sum(p.num_steps for p in self.dirs_to(point))\n",
    "\n",
    "    @cached_property\n",
    "    def adjacent_points(self) -> List[\"Point\"]:\n",
    "        return [self.apply(a) for a in ALL_DIRECTIONS]\n",
    "\n",
    "    @cached_property\n",
    "    def row(self) -> List[\"Point\"]:\n",
    "        return list(self._field.points[:, self.y])\n",
    "\n",
    "    @cached_property\n",
    "    def column(self) -> List[\"Point\"]:\n",
    "        return list(self._field.points[self.x, :])\n",
    "\n",
    "    @cached_call\n",
    "    def nearby_points(self, r: int) -> List[\"Point\"]:\n",
    "        if r > 1:\n",
    "            points = []\n",
    "            for p in self._field:\n",
    "                distance = self.distance_from(p)\n",
    "                if 0 < distance <= r:\n",
    "                    points.append(p)\n",
    "            return points\n",
    "        elif r == 1:\n",
    "            return self.adjacent_points\n",
    "\n",
    "        raise ValueError(\"Radius must be more or equal then 1\")\n",
    "\n",
    "    @cached_call\n",
    "    def dirs_to(self, point: \"Point\") -> List[\"PlanPath\"]:\n",
    "        dx, dy = self._field.swap(self._x - point.x, self._y - point.y)\n",
    "        ret = []\n",
    "        if dx:\n",
    "            ret.append(PlanPath(West, dx))\n",
    "        if dy:\n",
    "            ret.append(PlanPath(South, dy))\n",
    "        return ret\n",
    "\n",
    "\n",
    "class Field:\n",
    "    def __init__(self, size: int):\n",
    "        self._size = size\n",
    "        self._points = self.create_array(size)\n",
    "\n",
    "    def __iter__(self) -> Generator[Point, None, None]:\n",
    "        for row in self._points:\n",
    "            yield from row\n",
    "\n",
    "    def create_array(self, size: int) -> np.ndarray:\n",
    "        ar = np.zeros((size, size), dtype=Point)\n",
    "        for x in range(size):\n",
    "            for y in range(size):\n",
    "                point = Point(x, y, kore=0, field=self)\n",
    "                ar[x, y] = point\n",
    "        return ar\n",
    "\n",
    "    @property\n",
    "    def points(self) -> np.ndarray:\n",
    "        return self._points\n",
    "\n",
    "    def get_row(self, y: int, start: int, size: int) -> List[Point]:\n",
    "        if size < 0:\n",
    "            return self.get_row(y, start=start + size + 1, size=-size)[::-1]\n",
    "\n",
    "        ps = self._points\n",
    "        start %= self._size\n",
    "        out = []\n",
    "        while size > 0:\n",
    "            d = list(ps[slice(start, start + size), y])\n",
    "            size -= len(d)\n",
    "            start = 0\n",
    "            out += d\n",
    "        return out\n",
    "\n",
    "    def get_column(self, x: int, start: int, size: int) -> List[Point]:\n",
    "        if size < 0:\n",
    "            return self.get_column(x, start=start + size + 1, size=-size)[::-1]\n",
    "\n",
    "        ps = self._points\n",
    "        start %= self._size\n",
    "        out = []\n",
    "        while size > 0:\n",
    "            d = list(ps[x, slice(start, start + size)])\n",
    "            size -= len(d)\n",
    "            start = 0\n",
    "            out += d\n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return self._size\n",
    "\n",
    "    def swap(self, dx, dy):\n",
    "        size = self._size\n",
    "        if abs(dx) > size / 2:\n",
    "            dx -= np.sign(dx) * size\n",
    "        if abs(dy) > size / 2:\n",
    "            dy -= np.sign(dy) * size\n",
    "        return dx, dy\n",
    "\n",
    "    def __getitem__(self, item) -> Point:\n",
    "        x, y = item\n",
    "        return self._points[x % self._size, y % self._size]\n",
    "\n",
    "\n",
    "class PlanPath:\n",
    "    def __init__(self, direction: Action, num_steps: int = 0):\n",
    "        if direction == Convert:\n",
    "            self._direction = direction\n",
    "            self._num_steps = 0\n",
    "        elif num_steps > 0:\n",
    "            self._direction = direction\n",
    "            self._num_steps = num_steps\n",
    "        else:\n",
    "            self._direction = get_opposite_action(direction)\n",
    "            self._num_steps = -num_steps\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.to_str()\n",
    "\n",
    "    @property\n",
    "    def direction(self):\n",
    "        return self._direction\n",
    "\n",
    "    @property\n",
    "    def num_steps(self):\n",
    "        return self._num_steps\n",
    "\n",
    "    def to_str(self):\n",
    "        if self.direction == Convert:\n",
    "            return Convert.command\n",
    "        elif self.num_steps == 0:\n",
    "            return \"\"\n",
    "        elif self.num_steps == 1:\n",
    "            return self.direction.command\n",
    "        else:\n",
    "            return self.direction.command + str(self.num_steps - 1)\n",
    "\n",
    "    def reverse(self) -> \"PlanPath\":\n",
    "        return PlanPath(self.direction, -self.num_steps)\n",
    "\n",
    "\n",
    "class PlanRoute:\n",
    "    def __init__(self, paths: List[PlanPath]):\n",
    "        self._paths = self.simplify(paths)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.to_str()\n",
    "\n",
    "    def __add__(self, other: \"PlanRoute\") -> \"PlanRoute\":\n",
    "        return PlanRoute(self.paths + other.paths)\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self._paths)\n",
    "\n",
    "    @property\n",
    "    def paths(self):\n",
    "        return self._paths\n",
    "\n",
    "    @property\n",
    "    def num_steps(self):\n",
    "        return sum(x.num_steps for x in self._paths)\n",
    "\n",
    "    @classmethod\n",
    "    def simplify(cls, paths: List[PlanPath]):\n",
    "        if not paths:\n",
    "            return paths\n",
    "\n",
    "        new_paths = []\n",
    "        last_path = None\n",
    "        for p in paths:\n",
    "            if last_path and p.direction == last_path.direction:\n",
    "                new_paths[-1] = PlanPath(p.direction, p.num_steps + last_path.num_steps)\n",
    "            else:\n",
    "                last_path = p\n",
    "                new_paths.append(p)\n",
    "        return new_paths\n",
    "\n",
    "    def command_length(self):\n",
    "        return len(self.to_str())\n",
    "\n",
    "    def min_fleet_size(self):\n",
    "        return min_ship_count_for_flight_plan_len(self.command_length())\n",
    "\n",
    "    def reverse(self) -> \"PlanRoute\":\n",
    "        return PlanRoute([x.reverse() for x in self.paths])\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        actions = []\n",
    "        for p in self.paths:\n",
    "            actions += [p.direction for _ in range(p.num_steps)]\n",
    "        return actions\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, str_plan: str, current_direction: Action) -> \"PlanRoute\":\n",
    "        if current_direction not in ALL_DIRECTIONS:\n",
    "            raise ValueError(f\"Unknown direction `{current_direction}`\")\n",
    "\n",
    "        if not str_plan:\n",
    "            return PlanRoute([PlanPath(current_direction, np.inf)])\n",
    "\n",
    "        commands = []\n",
    "        for x in str_plan:\n",
    "            if x in COMMAND_TO_ACTION:\n",
    "                commands.append([])\n",
    "                commands[-1].append(x)\n",
    "            elif x.isdigit():\n",
    "                if not commands:\n",
    "                    commands = [[]]\n",
    "                commands[-1].append(x)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown command `{x}`.\")\n",
    "\n",
    "        paths = []\n",
    "        for i, p in enumerate(commands):\n",
    "            if i == 0 and p[0].isdigit():\n",
    "                action = current_direction\n",
    "                num_steps = int(\"\".join(p))\n",
    "                if num_steps == 0:\n",
    "                    continue\n",
    "            else:\n",
    "                action = COMMAND_TO_ACTION[p[0]]\n",
    "                if len(p) == 1:\n",
    "                    num_steps = 1\n",
    "                else:\n",
    "                    num_steps = int(\"\".join(p[1:])) + 1\n",
    "\n",
    "            paths.append(PlanPath(direction=action, num_steps=num_steps))\n",
    "            if action == Convert:\n",
    "                break\n",
    "\n",
    "        if not paths:\n",
    "            return PlanRoute([PlanPath(current_direction, np.inf)])\n",
    "\n",
    "        last_direction = paths[-1].direction\n",
    "        if last_direction != Convert:\n",
    "            paths[-1] = PlanPath(direction=last_direction, num_steps=np.inf)\n",
    "\n",
    "        return PlanRoute(paths)\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        s = \"\"\n",
    "        for a in self.paths[:-1]:\n",
    "            s += a.to_str()\n",
    "        s += self.paths[-1].direction.command\n",
    "        return s\n",
    "\n",
    "#%%writefile board.py\n",
    "\n",
    "\n",
    "# <--->\n",
    "'''\n",
    "from basic import (\n",
    "    Obj,\n",
    "    collection_rate_for_ship_count,\n",
    "    max_ships_to_spawn,\n",
    "    cached_property,\n",
    "    create_spawn_ships_command,\n",
    "    create_launch_fleet_command,\n",
    ")\n",
    "from geometry import (\n",
    "    Field,\n",
    "    Action,\n",
    "    Point,\n",
    "    North,\n",
    "    South,\n",
    "    Convert,\n",
    "    PlanPath,\n",
    "    PlanRoute,\n",
    "    GAME_ID_TO_ACTION,\n",
    ")\n",
    "'''\n",
    "\n",
    "class _ShipyardAction:\n",
    "    def to_str(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.to_str()\n",
    "\n",
    "\n",
    "class Spawn(_ShipyardAction):\n",
    "    def __init__(self, ship_count: int):\n",
    "        self.ship_count = ship_count\n",
    "\n",
    "    def to_str(self):\n",
    "        return create_spawn_ships_command(self.ship_count)\n",
    "\n",
    "\n",
    "class Launch(_ShipyardAction):\n",
    "    def __init__(self, ship_count: int, route: \"BoardRoute\"):\n",
    "        self.ship_count = ship_count\n",
    "        self.route = route\n",
    "\n",
    "    def to_str(self):\n",
    "        return create_launch_fleet_command(self.ship_count, self.route.plan.to_str())\n",
    "\n",
    "\n",
    "class DoNothing(_ShipyardAction):\n",
    "    def __repr__(self):\n",
    "        return \"Do nothing\"\n",
    "\n",
    "    def to_str(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BoardPath:\n",
    "    max_length = 32\n",
    "\n",
    "    def __init__(self, start: \"Point\", plan: PlanPath):\n",
    "        assert plan.num_steps > 0 or plan.direction == Convert\n",
    "\n",
    "        self._plan = plan\n",
    "\n",
    "        field = start.field\n",
    "        x, y = start.x, start.y\n",
    "        if np.isfinite(plan.num_steps):\n",
    "            n = plan.num_steps + 1\n",
    "        else:\n",
    "            n = self.max_length\n",
    "        action = plan.direction\n",
    "\n",
    "        if plan.direction == Convert:\n",
    "            self._track = []\n",
    "            self._start = start\n",
    "            self._end = start\n",
    "            self._build_shipyard = True\n",
    "            return\n",
    "\n",
    "        if action in (North, South):\n",
    "            track = field.get_column(x, start=y, size=n * action.dy)\n",
    "        else:\n",
    "            track = field.get_row(y, start=x, size=n * action.dx)\n",
    "\n",
    "        self._track = track[1:]\n",
    "        self._start = start\n",
    "        self._end = track[-1]\n",
    "        self._build_shipyard = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        start, end = self.start, self.end\n",
    "        return f\"({start.x}, {start.y}) -> ({end.x}, {end.y})\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._track)\n",
    "\n",
    "    @property\n",
    "    def plan(self):\n",
    "        return self._plan\n",
    "\n",
    "    @property\n",
    "    def points(self):\n",
    "        return self._track\n",
    "\n",
    "    @property\n",
    "    def start(self):\n",
    "        return self._start\n",
    "\n",
    "    @property\n",
    "    def end(self):\n",
    "        return self._end\n",
    "\n",
    "\n",
    "class BoardRoute:\n",
    "    def __init__(self, start: \"Point\", plan: \"PlanRoute\"):\n",
    "        paths = []\n",
    "        for p in plan.paths:\n",
    "            path = BoardPath(start, p)\n",
    "            start = path.end\n",
    "            paths.append(path)\n",
    "\n",
    "        self._plan = plan\n",
    "        self._paths = paths\n",
    "        self._start = paths[0].start\n",
    "        self._end = paths[-1].end\n",
    "\n",
    "    def __repr__(self):\n",
    "        points = []\n",
    "        for p in self._paths:\n",
    "            points.append(p.start)\n",
    "        points.append(self.end)\n",
    "        return \" -> \".join([f\"({p.x}, {p.y})\" for p in points])\n",
    "\n",
    "    def __iter__(self) -> Generator[\"Point\", None, None]:\n",
    "        for p in self._paths:\n",
    "            yield from p.points\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(x) for x in self._paths)\n",
    "\n",
    "    def points(self) -> List[\"Point\"]:\n",
    "        points = []\n",
    "        for p in self._paths:\n",
    "            points += p.points\n",
    "        return points\n",
    "\n",
    "    @property\n",
    "    def plan(self) -> PlanRoute:\n",
    "        return self._plan\n",
    "\n",
    "    def command(self) -> str:\n",
    "        return self.plan.to_str()\n",
    "\n",
    "    @property\n",
    "    def paths(self) -> List[BoardPath]:\n",
    "        return self._paths\n",
    "\n",
    "    @property\n",
    "    def start(self) -> \"Point\":\n",
    "        return self._start\n",
    "\n",
    "    @property\n",
    "    def end(self) -> \"Point\":\n",
    "        return self._end\n",
    "\n",
    "    def command_length(self) -> int:\n",
    "        return len(self.command())\n",
    "\n",
    "    def last_action(self):\n",
    "        return self.paths[-1].plan.direction\n",
    "\n",
    "    def expected_kore(self, board: \"Board\", ship_count: int):\n",
    "        rate = collection_rate_for_ship_count(ship_count)\n",
    "        if rate <= 0:\n",
    "            return 0\n",
    "\n",
    "        point_to_time = {}\n",
    "        point_to_kore = {}\n",
    "        for t, p in enumerate(self):\n",
    "            point_to_time[p] = t\n",
    "            point_to_kore[p] = p.kore\n",
    "\n",
    "        for f in board.fleets:\n",
    "            for t, p in enumerate(f.route):\n",
    "                if p in point_to_time and t < point_to_time[p]:\n",
    "                    point_to_kore[p] *= f.collection_rate\n",
    "\n",
    "        return sum([kore * rate for kore in point_to_kore.values()])\n",
    "\n",
    "\n",
    "class PositionObj(Obj):\n",
    "    def __init__(self, *args, point: Point, player_id: int, board: \"Board\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._point = point\n",
    "        self._player_id = player_id\n",
    "        self._board = board\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(id={self._game_id}, position={self._point}, player={self._player_id})\"\n",
    "\n",
    "    def dirs_to(self, obj: Union[\"PositionObj\", Point]):\n",
    "        if isinstance(obj, Point):\n",
    "            return self._point.dirs_to(obj)\n",
    "        return self._point.dirs_to(obj.point)\n",
    "\n",
    "    def distance_from(self, obj: Union[\"PositionObj\", Point]) -> int:\n",
    "        if isinstance(obj, Point):\n",
    "            return self._point.distance_from(obj)\n",
    "        return self._point.distance_from(obj.point)\n",
    "\n",
    "    @property\n",
    "    def board(self) -> \"Board\":\n",
    "        return self._board\n",
    "\n",
    "    @property\n",
    "    def point(self) -> Point:\n",
    "        return self._point\n",
    "\n",
    "    @property\n",
    "    def player_id(self):\n",
    "        return self._player_id\n",
    "\n",
    "    @property\n",
    "    def player(self) -> \"Player\":\n",
    "        return self.board.get_player(self.player_id)\n",
    "\n",
    "\n",
    "class Shipyard(PositionObj):\n",
    "    def __init__(self, *args, ship_count: int, turns_controlled: int, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._ship_count = ship_count\n",
    "        self._turns_controlled = turns_controlled\n",
    "        self._guard_ship_count = 0\n",
    "        self.action: Optional[_ShipyardAction] = None\n",
    "\n",
    "    @property\n",
    "    def turns_controlled(self):\n",
    "        return self._turns_controlled\n",
    "\n",
    "    @property\n",
    "    def max_ships_to_spawn(self) -> int:\n",
    "        return max_ships_to_spawn(self._turns_controlled)\n",
    "\n",
    "    @property\n",
    "    def ship_count(self):\n",
    "        return self._ship_count\n",
    "\n",
    "    @property\n",
    "    def available_ship_count(self):\n",
    "        return self._ship_count - self._guard_ship_count\n",
    "\n",
    "    @property\n",
    "    def guard_ship_count(self):\n",
    "        return self._guard_ship_count\n",
    "\n",
    "    def set_guard_ship_count(self, ship_count):\n",
    "        assert ship_count <= self._ship_count\n",
    "        self._guard_ship_count = ship_count\n",
    "\n",
    "    @cached_property\n",
    "    def incoming_allied_fleets(self) -> List[\"Fleet\"]:\n",
    "        fleets = []\n",
    "        for f in self.board.fleets:\n",
    "            if f.player_id == self.player_id and f.route.end == self.point:\n",
    "                fleets.append(f)\n",
    "        return fleets\n",
    "\n",
    "    @cached_property\n",
    "    def incoming_hostile_fleets(self) -> List[\"Fleet\"]:\n",
    "        fleets = []\n",
    "        for f in self.board.fleets:\n",
    "            if f.player_id != self.player_id and f.route.end == self.point:\n",
    "                fleets.append(f)\n",
    "        return fleets\n",
    "\n",
    "\n",
    "class Fleet(PositionObj):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        ship_count: int,\n",
    "        kore: int,\n",
    "        route: BoardRoute,\n",
    "        direction: Action,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        assert ship_count > 0\n",
    "        assert kore >= 0\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self._ship_count = ship_count\n",
    "        self._kore = kore\n",
    "        self._direction = direction\n",
    "        self._route = route\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        if self.ship_count != other.ship_count:\n",
    "            return self.ship_count > other.ship_count\n",
    "        if self.kore != other.kore:\n",
    "            return self.kore > other.kore\n",
    "        return self.direction.game_id > other.direction.game_id\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return other.__gt__(self)\n",
    "\n",
    "    @property\n",
    "    def ship_count(self):\n",
    "        return self._ship_count\n",
    "\n",
    "    @property\n",
    "    def kore(self):\n",
    "        return self._kore\n",
    "\n",
    "    @property\n",
    "    def route(self):\n",
    "        return self._route\n",
    "\n",
    "    @property\n",
    "    def eta(self):\n",
    "        return len(self._route)\n",
    "\n",
    "    def set_route(self, route: BoardRoute):\n",
    "        self._route = route\n",
    "\n",
    "    @property\n",
    "    def direction(self):\n",
    "        return self._direction\n",
    "\n",
    "    @property\n",
    "    def collection_rate(self) -> float:\n",
    "        return collection_rate_for_ship_count(self._ship_count)\n",
    "\n",
    "    def expected_kore(self):\n",
    "        return self._kore + self._route.expected_kore(self._board, self._ship_count)\n",
    "\n",
    "    def cost(self):\n",
    "        return self.board.spawn_cost * self.ship_count\n",
    "\n",
    "    def value(self):\n",
    "        return self.kore / self.cost()\n",
    "\n",
    "    def expected_value(self):\n",
    "        return self.expected_kore() / self.cost()\n",
    "\n",
    "\n",
    "class FleetPointer:\n",
    "    def __init__(self, fleet: Fleet):\n",
    "        self.obj = fleet\n",
    "        self.point = fleet.point\n",
    "        self.is_active = True\n",
    "        self._paths = []\n",
    "        self._points = self.points()\n",
    "\n",
    "    def points(self):\n",
    "        for path in self.obj.route.paths:\n",
    "            self._paths.append([path.plan.direction, 0])\n",
    "            for point in path.points:\n",
    "                self._paths[-1][1] += 1\n",
    "                yield point\n",
    "\n",
    "    def update(self):\n",
    "        if not self.is_active:\n",
    "            self.point = None\n",
    "            return\n",
    "        try:\n",
    "            self.point = next(self._points)\n",
    "        except StopIteration:\n",
    "            self.point = None\n",
    "            self.is_active = False\n",
    "\n",
    "    def current_route(self):\n",
    "        plan = PlanRoute([PlanPath(d, n) for d, n in self._paths])\n",
    "        return BoardRoute(self.obj.point, plan)\n",
    "\n",
    "\n",
    "class Player(Obj):\n",
    "    def __init__(self, *args, kore: float, board: \"Board\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._kore = kore\n",
    "        self._board = board\n",
    "\n",
    "    @property\n",
    "    def kore(self):\n",
    "        return self._kore\n",
    "\n",
    "    def fleet_kore(self):\n",
    "        return sum(x.kore for x in self.fleets)\n",
    "\n",
    "    def fleet_expected_kore(self):\n",
    "        return sum(x.expected_kore() for x in self.fleets)\n",
    "\n",
    "    def is_active(self):\n",
    "        return len(self.fleets) > 0 or len(self.shipyards) > 0\n",
    "\n",
    "    @property\n",
    "    def board(self):\n",
    "        return self._board\n",
    "\n",
    "    def _get_objects(self, name):\n",
    "        d = []\n",
    "        for x in self._board.__getattribute__(name):\n",
    "            if x.player_id == self.game_id:\n",
    "                d.append(x)\n",
    "        return d\n",
    "\n",
    "    @cached_property\n",
    "    def fleets(self) -> List[Fleet]:\n",
    "        return self._get_objects(\"fleets\")\n",
    "\n",
    "    @cached_property\n",
    "    def shipyards(self) -> List[Shipyard]:\n",
    "        return self._get_objects(\"shipyards\")\n",
    "\n",
    "    @cached_property\n",
    "    def ship_count(self) -> int:\n",
    "        return sum(x.ship_count for x in itertools.chain(self.fleets, self.shipyards))\n",
    "\n",
    "    @cached_property\n",
    "    def opponents(self) -> List[\"Player\"]:\n",
    "        return [x for x in self.board.players if x != self]\n",
    "\n",
    "    @cached_property\n",
    "    def expected_fleets_positions(self) -> Dict[int, Dict[Point, int]]:\n",
    "        \"\"\"\n",
    "        time -> point -> fleet\n",
    "        \"\"\"\n",
    "        time_to_fleet_positions = defaultdict(dict)\n",
    "        for f in self.fleets:\n",
    "            for time, point in enumerate(f.route):\n",
    "                time_to_fleet_positions[time][point] = f\n",
    "        return time_to_fleet_positions\n",
    "\n",
    "    @cached_property\n",
    "    def expected_dmg_positions(self) -> Dict[int, Dict[Point, int]]:\n",
    "        \"\"\"\n",
    "        time -> point -> dmg\n",
    "        \"\"\"\n",
    "        time_to_dmg_positions = defaultdict(dict)\n",
    "        for f in self.fleets:\n",
    "            for time, point in enumerate(f.route):\n",
    "                for adjacent_point in point.adjacent_points:\n",
    "                    point_to_dmg = time_to_dmg_positions[time]\n",
    "                    if adjacent_point not in point_to_dmg:\n",
    "                        point_to_dmg[adjacent_point] = 0\n",
    "                    point_to_dmg[adjacent_point] += f.ship_count\n",
    "        return time_to_dmg_positions\n",
    "\n",
    "    def actions(self):\n",
    "        if self.available_kore() < 0:\n",
    "            logger.warning(\"Negative balance. Some ships will not spawn.\")\n",
    "\n",
    "        shipyard_id_to_action = {}\n",
    "        for sy in self.shipyards:\n",
    "            if not sy.action or isinstance(sy.action, DoNothing):\n",
    "                continue\n",
    "\n",
    "            shipyard_id_to_action[sy.game_id] = sy.action.to_str()\n",
    "        return shipyard_id_to_action\n",
    "\n",
    "    def spawn_ship_count(self):\n",
    "        return sum(\n",
    "            x.action.ship_count for x in self.shipyards if isinstance(x.action, Spawn)\n",
    "        )\n",
    "\n",
    "    def need_kore_for_spawn(self):\n",
    "        return self.board.spawn_cost * self.spawn_ship_count()\n",
    "\n",
    "    def available_kore(self):\n",
    "        return self._kore - self.need_kore_for_spawn()\n",
    "\n",
    "\n",
    "_FIELD = None\n",
    "\n",
    "\n",
    "class Board:\n",
    "    def __init__(self, obs, conf):\n",
    "        self._conf = Configuration(conf)\n",
    "        self._step = obs[\"step\"]\n",
    "\n",
    "        global _FIELD\n",
    "        if _FIELD is None or self._step == 0:\n",
    "            _FIELD = Field(self._conf.size)\n",
    "        else:\n",
    "            assert _FIELD.size == self._conf.size\n",
    "\n",
    "        self._field: Field = _FIELD\n",
    "\n",
    "        id_to_point = {x.game_id: x for x in self._field}\n",
    "\n",
    "        for point_id, kore in enumerate(obs[\"kore\"]):\n",
    "            point = id_to_point[point_id]\n",
    "            point.set_kore(kore)\n",
    "\n",
    "        self._players = []\n",
    "        self._fleets = []\n",
    "        self._shipyards = []\n",
    "        for player_id, player_data in enumerate(obs[\"players\"]):\n",
    "            player_kore, player_shipyards, player_fleets = player_data\n",
    "            player = Player(game_id=player_id, kore=player_kore, board=self)\n",
    "            self._players.append(player)\n",
    "\n",
    "            for fleet_id, fleet_data in player_fleets.items():\n",
    "                point_id, kore, ship_count, direction, flight_plan = fleet_data\n",
    "                position = id_to_point[point_id]\n",
    "                direction = GAME_ID_TO_ACTION[direction]\n",
    "                if ship_count < self.shipyard_cost and Convert.command in flight_plan:\n",
    "                    # can't convert\n",
    "                    flight_plan = \"\".join(\n",
    "                        [x for x in flight_plan if x != Convert.command]\n",
    "                    )\n",
    "                plan = PlanRoute.from_str(flight_plan, direction)\n",
    "                route = BoardRoute(position, plan)\n",
    "                fleet = Fleet(\n",
    "                    game_id=fleet_id,\n",
    "                    point=position,\n",
    "                    player_id=player_id,\n",
    "                    ship_count=ship_count,\n",
    "                    kore=kore,\n",
    "                    route=route,\n",
    "                    direction=direction,\n",
    "                    board=self,\n",
    "                )\n",
    "                self._fleets.append(fleet)\n",
    "\n",
    "            for shipyard_id, shipyard_data in player_shipyards.items():\n",
    "                point_id, ship_count, turns_controlled = shipyard_data\n",
    "                position = id_to_point[point_id]\n",
    "                shipyard = Shipyard(\n",
    "                    game_id=shipyard_id,\n",
    "                    point=position,\n",
    "                    player_id=player_id,\n",
    "                    ship_count=ship_count,\n",
    "                    turns_controlled=turns_controlled,\n",
    "                    board=self,\n",
    "                )\n",
    "                self._shipyards.append(shipyard)\n",
    "\n",
    "        self._players = [x for x in self._players if x.is_active()]\n",
    "\n",
    "        self._update_fleets_destination()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._field[item]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._field.__iter__()\n",
    "\n",
    "    @property\n",
    "    def field(self):\n",
    "        return self._field\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._field.size\n",
    "\n",
    "    @property\n",
    "    def step(self):\n",
    "        return self._step\n",
    "\n",
    "    @property\n",
    "    def steps_left(self):\n",
    "        return self._conf.episode_steps - self._step - 1\n",
    "\n",
    "    @property\n",
    "    def shipyard_cost(self):\n",
    "        return self._conf.convert_cost\n",
    "\n",
    "    @property\n",
    "    def spawn_cost(self):\n",
    "        return self._conf.spawn_cost\n",
    "\n",
    "    @property\n",
    "    def regen_rate(self):\n",
    "        return self._conf.regen_rate\n",
    "\n",
    "    @property\n",
    "    def max_cell_kore(self):\n",
    "        return self._conf.max_cell_kore\n",
    "\n",
    "    @property\n",
    "    def players(self) -> List[Player]:\n",
    "        return self._players\n",
    "\n",
    "    @property\n",
    "    def fleets(self) -> List[Fleet]:\n",
    "        return self._fleets\n",
    "\n",
    "    @property\n",
    "    def shipyards(self) -> List[Shipyard]:\n",
    "        return self._shipyards\n",
    "\n",
    "    def get_player(self, game_id) -> Player:\n",
    "        for p in self._players:\n",
    "            if p.game_id == game_id:\n",
    "                return p\n",
    "        raise KeyError(f\"Player `{game_id}` doas not exists.\")\n",
    "\n",
    "    def get_obj_at_point(self, point: Point) -> Optional[Union[Fleet, Shipyard]]:\n",
    "        for x in itertools.chain(self.fleets, self.shipyards):\n",
    "            if x.point == point:\n",
    "                return x\n",
    "\n",
    "    def _update_fleets_destination(self):\n",
    "        \"\"\"\n",
    "        trying to predict future positions\n",
    "        very inaccurate\n",
    "        \"\"\"\n",
    "\n",
    "        shipyard_positions = {x.point for x in self.shipyards}\n",
    "\n",
    "        fleets = [FleetPointer(f) for f in self.fleets]\n",
    "\n",
    "        while any(x.is_active for x in fleets):\n",
    "            for f in fleets:\n",
    "                f.update()\n",
    "\n",
    "            # fleet to shipyard\n",
    "            for f in fleets:\n",
    "                if f.point in shipyard_positions:\n",
    "                    f.is_active = False\n",
    "\n",
    "            # allied fleets\n",
    "            for player in self.players:\n",
    "                point_to_fleets = defaultdict(list)\n",
    "                for f in fleets:\n",
    "                    if f.is_active and f.obj.player_id == player.game_id:\n",
    "                        point_to_fleets[f.point].append(f)\n",
    "                for point_fleets in point_to_fleets.values():\n",
    "                    if len(point_fleets) > 1:\n",
    "                        for f in sorted(point_fleets, key=lambda x: x.obj)[:-1]:\n",
    "                            f.is_active = False\n",
    "\n",
    "            # fleet to fleet\n",
    "            point_to_fleets = defaultdict(list)\n",
    "            for f in fleets:\n",
    "                if f.is_active:\n",
    "                    point_to_fleets[f.point].append(f)\n",
    "            for point_fleets in point_to_fleets.values():\n",
    "                if len(point_fleets) > 1:\n",
    "                    for f in sorted(point_fleets, key=lambda x: x.obj)[:-1]:\n",
    "                        f.is_active = False\n",
    "\n",
    "            # adjacent damage\n",
    "            point_to_fleet = {}\n",
    "            for f in fleets:\n",
    "                if f.is_active:\n",
    "                    point_to_fleet[f.point] = f\n",
    "\n",
    "            point_to_dmg = defaultdict(int)\n",
    "            for point, fleet in point_to_fleet.items():\n",
    "                for p in point.adjacent_points:\n",
    "                    if p in point_to_fleet:\n",
    "                        adjacent_fleet = point_to_fleet[p]\n",
    "                        if adjacent_fleet.obj.player_id != fleet.obj.player_id:\n",
    "                            point_to_dmg[p] += fleet.obj.ship_count\n",
    "\n",
    "            for point, fleet in point_to_fleet.items():\n",
    "                dmg = point_to_dmg[point]\n",
    "                if fleet.obj.ship_count <= dmg:\n",
    "                    fleet.is_active = False\n",
    "\n",
    "        for f in fleets:\n",
    "            f.obj.set_route(f.current_route())\n",
    "\n",
    "\n",
    "##control.py\n",
    "#%%writefile control.py\n",
    "\n",
    "# <--->\n",
    "'''\n",
    "from geometry import PlanRoute\n",
    "from board import Player, Launch, Spawn, Fleet, FleetPointer, BoardRoute\n",
    "from helpers import is_invitable_victory, find_shortcut_routes\n",
    "from logger import logger\n",
    "'''\n",
    "# <--->\n",
    "\n",
    "\n",
    "def direct_attack(agent: Player, max_distance: int = 10):\n",
    "    board = agent.board\n",
    "\n",
    "    max_distance = min(board.steps_left, max_distance)\n",
    "\n",
    "    targets = []\n",
    "    for x in agent.opponents:\n",
    "        for sy in x.shipyards:\n",
    "            for fleet in sy.incoming_allied_fleets:\n",
    "                if fleet.expected_value() > 0.5:\n",
    "                    targets.append(fleet)\n",
    "\n",
    "    if not targets:\n",
    "        return\n",
    "\n",
    "    shipyards = [\n",
    "        x for x in agent.shipyards if x.available_ship_count > 0 and not x.action\n",
    "    ]\n",
    "    if not shipyards:\n",
    "        return\n",
    "\n",
    "    point_to_closest_shipyard = {}\n",
    "    for p in board:\n",
    "        closest_shipyard = None\n",
    "        min_distance = board.size\n",
    "        for sy in agent.shipyards:\n",
    "            distance = sy.point.distance_from(p)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_shipyard = sy\n",
    "        point_to_closest_shipyard[p] = closest_shipyard.point\n",
    "\n",
    "    opponent_shipyard_points = {x.point for x in board.shipyards if x.player_id != agent.game_id}\n",
    "    for t in targets:\n",
    "        min_ships_to_send = int(t.ship_count * 1.2)\n",
    "        attacked = False\n",
    "\n",
    "        for sy in shipyards:\n",
    "            if sy.action or sy.available_ship_count < min_ships_to_send:\n",
    "                continue\n",
    "\n",
    "            num_ships_to_launch = sy.available_ship_count\n",
    "\n",
    "            for target_time, target_point in enumerate(t.route, 1):\n",
    "                if target_time > max_distance:\n",
    "                    continue\n",
    "\n",
    "                if sy.point.distance_from(target_point) != target_time:\n",
    "                    continue\n",
    "\n",
    "                paths = sy.point.dirs_to(target_point)\n",
    "                random.shuffle(paths)\n",
    "                plan = PlanRoute(paths)\n",
    "                destination = point_to_closest_shipyard[target_point]\n",
    "\n",
    "                paths = target_point.dirs_to(destination)\n",
    "                random.shuffle(paths)\n",
    "                plan += PlanRoute(paths)\n",
    "                if num_ships_to_launch < plan.min_fleet_size():\n",
    "                    continue\n",
    "\n",
    "                route = BoardRoute(sy.point, plan)\n",
    "\n",
    "                if any(x in opponent_shipyard_points for x in route.points()):\n",
    "                    continue\n",
    "\n",
    "                if is_intercept_direct_attack_route(route, agent, direct_attack_fleet=t):\n",
    "                    continue\n",
    "\n",
    "                logger.info(\n",
    "                    f\"Direct attack {sy.point}->{target_point}, distance={target_time}\"\n",
    "                )\n",
    "                sy.action = Launch(num_ships_to_launch, route)\n",
    "                attacked = True\n",
    "                break\n",
    "\n",
    "            if attacked:\n",
    "                break\n",
    "\n",
    "\n",
    "def is_intercept_direct_attack_route(\n",
    "    route: BoardRoute, player: Player, direct_attack_fleet: Fleet\n",
    "):\n",
    "    board = player.board\n",
    "\n",
    "    fleets = [FleetPointer(f) for f in board.fleets if f != direct_attack_fleet]\n",
    "\n",
    "    for point in route.points()[:-1]:\n",
    "        for fleet in fleets:\n",
    "            fleet.update()\n",
    "\n",
    "            if fleet.point is None:\n",
    "                continue\n",
    "\n",
    "            if fleet.point == point:\n",
    "                return True\n",
    "\n",
    "            if fleet.obj.player_id != player.game_id:\n",
    "                for p in fleet.point.adjacent_points:\n",
    "                    if p == point:\n",
    "                        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def adjacent_attack(agent: Player, max_distance: int = 10):\n",
    "    board = agent.board\n",
    "\n",
    "    max_distance = min(board.steps_left, max_distance)\n",
    "\n",
    "    targets = _find_adjacent_targets(agent, max_distance)\n",
    "    if not targets:\n",
    "        return\n",
    "\n",
    "    shipyards = [\n",
    "        x for x in agent.shipyards if x.available_ship_count > 0 and not x.action\n",
    "    ]\n",
    "    if not shipyards:\n",
    "        return\n",
    "\n",
    "    fleets_to_be_attacked = set()\n",
    "    for t in sorted(targets, key=lambda x: (-len(x[\"fleets\"]), x[\"time\"])):\n",
    "        target_point = t[\"point\"]\n",
    "        target_time = t[\"time\"]\n",
    "        target_fleets = t[\"fleets\"]\n",
    "        if any(x in fleets_to_be_attacked for x in target_fleets):\n",
    "            continue\n",
    "\n",
    "        for sy in shipyards:\n",
    "            if sy.action:\n",
    "                continue\n",
    "\n",
    "            distance = sy.distance_from(target_point)\n",
    "            if distance > target_time:\n",
    "                continue\n",
    "            min_ship_count = min(x.ship_count for x in target_fleets)\n",
    "            num_ships_to_send = min(sy.available_ship_count, min_ship_count)\n",
    "\n",
    "            routes = find_shortcut_routes(\n",
    "                board,\n",
    "                sy.point,\n",
    "                target_point,\n",
    "                agent,\n",
    "                num_ships_to_send,\n",
    "                route_distance=target_time,\n",
    "            )\n",
    "            if not routes:\n",
    "                continue\n",
    "\n",
    "            route = random.choice(routes)\n",
    "            logger.info(\n",
    "                f\"Adjacent attack {sy.point}->{target_point}, distance={distance}, target_time={target_time}\"\n",
    "            )\n",
    "            sy.action = Launch(num_ships_to_send, route)\n",
    "            for fleet in target_fleets:\n",
    "                fleets_to_be_attacked.add(fleet)\n",
    "            break\n",
    "\n",
    "\n",
    "def _find_adjacent_targets(agent: Player, max_distance: int = 5):\n",
    "    board = agent.board\n",
    "    shipyards_points = {x.point for x in board.shipyards}\n",
    "    fleets = [FleetPointer(f) for f in board.fleets]\n",
    "    if len(fleets) < 2:\n",
    "        return []\n",
    "\n",
    "    time = 0\n",
    "    targets = []\n",
    "    while any(x.is_active for x in fleets) and time <= max_distance:\n",
    "        time += 1\n",
    "\n",
    "        for f in fleets:\n",
    "            f.update()\n",
    "\n",
    "        point_to_fleet = {\n",
    "            x.point: x.obj\n",
    "            for x in fleets\n",
    "            if x.is_active and x.point not in shipyards_points\n",
    "        }\n",
    "\n",
    "        for point in board:\n",
    "            if point in point_to_fleet or point in shipyards_points:\n",
    "                continue\n",
    "\n",
    "            adjacent_fleets = [\n",
    "                point_to_fleet[x] for x in point.adjacent_points if x in point_to_fleet\n",
    "            ]\n",
    "            if len(adjacent_fleets) < 2:\n",
    "                continue\n",
    "\n",
    "            if any(x.player_id == agent.game_id for x in adjacent_fleets):\n",
    "                continue\n",
    "\n",
    "            targets.append({\"point\": point, \"time\": time, \"fleets\": adjacent_fleets})\n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "def _need_more_ships(agent: Player, ship_count: int):\n",
    "    board = agent.board\n",
    "    if board.steps_left < 10:\n",
    "        return False\n",
    "    if ship_count > _max_ships_to_control(agent):\n",
    "        return False\n",
    "    if board.steps_left < 50 and is_invitable_victory(agent):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _max_ships_to_control(agent: Player):\n",
    "    return max(100, 3 * sum(x.ship_count for x in agent.opponents))\n",
    "\n",
    "\n",
    "def greedy_spawn(agent: Player):\n",
    "    board = agent.board\n",
    "\n",
    "    if not _need_more_ships(agent, agent.ship_count):\n",
    "        return\n",
    "\n",
    "    ship_count = agent.ship_count\n",
    "    max_ship_count = _max_ships_to_control(agent)\n",
    "    for shipyard in agent.shipyards:\n",
    "        if shipyard.action:\n",
    "            continue\n",
    "\n",
    "        if shipyard.ship_count > agent.ship_count * 0.2 / len(agent.shipyards):\n",
    "            continue\n",
    "\n",
    "        num_ships_to_spawn = shipyard.max_ships_to_spawn\n",
    "        if int(agent.available_kore() // board.spawn_cost) >= num_ships_to_spawn:\n",
    "            shipyard.action = Spawn(num_ships_to_spawn)\n",
    "\n",
    "        ship_count += num_ships_to_spawn\n",
    "        if ship_count > max_ship_count:\n",
    "            return\n",
    "\n",
    "\n",
    "def spawn(agent: Player):\n",
    "    board = agent.board\n",
    "\n",
    "    if not _need_more_ships(agent, agent.ship_count):\n",
    "        return\n",
    "\n",
    "    ship_count = agent.ship_count\n",
    "    max_ship_count = _max_ships_to_control(agent)\n",
    "    for shipyard in agent.shipyards:\n",
    "        if shipyard.action:\n",
    "            continue\n",
    "        num_ships_to_spawn = min(\n",
    "            int(agent.available_kore() // board.spawn_cost),\n",
    "            shipyard.max_ships_to_spawn,\n",
    "        )\n",
    "        if num_ships_to_spawn:\n",
    "            shipyard.action = Spawn(num_ships_to_spawn)\n",
    "            ship_count += num_ships_to_spawn\n",
    "            if ship_count > max_ship_count:\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "###defence.py\n",
    "#%%writefile defence.py\n",
    "\n",
    "# <--->\n",
    "\"\"\"\n",
    "from board import Spawn, Player, Launch\n",
    "from helpers import find_shortcut_routes\n",
    "from logger import logger\n",
    "\"\"\"\n",
    "# <--->\n",
    "\n",
    "\n",
    "def defend_shipyards(agent: Player):\n",
    "    board = agent.board\n",
    "\n",
    "    need_help_shipyards = []\n",
    "    for sy in agent.shipyards:\n",
    "        if sy.action:\n",
    "            continue\n",
    "\n",
    "        incoming_hostile_fleets = sy.incoming_hostile_fleets\n",
    "        incoming_allied_fleets = sy.incoming_allied_fleets\n",
    "\n",
    "        if not incoming_hostile_fleets:\n",
    "            continue\n",
    "\n",
    "        incoming_hostile_power = sum(x.ship_count for x in incoming_hostile_fleets)\n",
    "        incoming_hostile_time = min(x.eta for x in incoming_hostile_fleets)\n",
    "        incoming_allied_power = sum(\n",
    "            x.ship_count\n",
    "            for x in incoming_allied_fleets\n",
    "            if x.eta < incoming_hostile_time\n",
    "        )\n",
    "\n",
    "        ships_needed = incoming_hostile_power - incoming_allied_power\n",
    "        if sy.ship_count > ships_needed:\n",
    "            sy.set_guard_ship_count(min(sy.ship_count, int(ships_needed * 1.1)))\n",
    "            continue\n",
    "\n",
    "        # spawn as much as possible\n",
    "        num_ships_to_spawn = min(\n",
    "            int(agent.available_kore() // board.spawn_cost), sy.max_ships_to_spawn\n",
    "        )\n",
    "        if num_ships_to_spawn:\n",
    "            logger.debug(f\"Spawn ships to protect shipyard {sy.point}\")\n",
    "            sy.action = Spawn(num_ships_to_spawn)\n",
    "\n",
    "        need_help_shipyards.append(sy)\n",
    "\n",
    "    for sy in need_help_shipyards:\n",
    "        incoming_hostile_fleets = sy.incoming_hostile_fleets\n",
    "        incoming_hostile_time = min(x.eta for x in incoming_hostile_fleets)\n",
    "\n",
    "        for other_sy in agent.shipyards:\n",
    "            if other_sy == sy or other_sy.action or not other_sy.available_ship_count:\n",
    "                continue\n",
    "\n",
    "            distance = other_sy.distance_from(sy)\n",
    "            if distance == incoming_hostile_time - 1:\n",
    "                routes = find_shortcut_routes(\n",
    "                    board, other_sy.point, sy.point, agent, other_sy.ship_count\n",
    "                )\n",
    "                if routes:\n",
    "                    logger.info(f\"Send reinforcements {other_sy.point}->{sy.point}\")\n",
    "                    other_sy.action = Launch(\n",
    "                        other_sy.available_ship_count, random.choice(routes)\n",
    "                    )\n",
    "            elif distance < incoming_hostile_time - 1:\n",
    "                other_sy.set_guard_ship_count(other_sy.ship_count)\n",
    "###                \n",
    "# <--->\n",
    "\"\"\"\n",
    "from basic import min_ship_count_for_flight_plan_len\n",
    "from geometry import Point, Convert, PlanRoute, PlanPath\n",
    "from board import Player, BoardRoute, Launch\n",
    "\"\"\"\n",
    "# <--->\n",
    "\n",
    "\n",
    "def expand(player: Player):\n",
    "    board = player.board\n",
    "    num_shipyards_to_create = need_more_shipyards(player)\n",
    "    if not num_shipyards_to_create:\n",
    "        return\n",
    "\n",
    "    shipyard_positions = {x.point for x in board.shipyards}\n",
    "\n",
    "    shipyard_to_point = find_best_position_for_shipyards(player)\n",
    "\n",
    "    shipyard_count = 0\n",
    "    for shipyard, target in shipyard_to_point.items():\n",
    "        if shipyard_count >= num_shipyards_to_create:\n",
    "            break\n",
    "\n",
    "        if shipyard.available_ship_count < board.shipyard_cost or shipyard.action:\n",
    "            continue\n",
    "\n",
    "        incoming_hostile_fleets = shipyard.incoming_hostile_fleets\n",
    "        if incoming_hostile_fleets:\n",
    "            continue\n",
    "\n",
    "        target_distance = shipyard.distance_from(target)\n",
    "\n",
    "        routes = []\n",
    "        for p in board:\n",
    "            if p in shipyard_positions:\n",
    "                continue\n",
    "\n",
    "            distance = shipyard.distance_from(p) + p.distance_from(target)\n",
    "            if distance > target_distance:\n",
    "                continue\n",
    "\n",
    "            plan = PlanRoute(shipyard.dirs_to(p) + p.dirs_to(target))\n",
    "            route = BoardRoute(shipyard.point, plan)\n",
    "\n",
    "            if shipyard.available_ship_count < min_ship_count_for_flight_plan_len(\n",
    "                len(route.plan.to_str()) + 1\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            route_points = route.points()\n",
    "            if any(x in shipyard_positions for x in route_points):\n",
    "                continue\n",
    "\n",
    "            if not is_safety_route_to_convert(route_points, player):\n",
    "                continue\n",
    "\n",
    "            routes.append(route)\n",
    "\n",
    "        if routes:\n",
    "            route = random.choice(routes)\n",
    "            route = BoardRoute(\n",
    "                shipyard.point, route.plan + PlanRoute([PlanPath(Convert)])\n",
    "            )\n",
    "            shipyard.action = Launch(shipyard.available_ship_count, route)\n",
    "            shipyard_count += 1\n",
    "\n",
    "\n",
    "def find_best_position_for_shipyards(player: Player):\n",
    "    board = player.board\n",
    "    shipyards = board.shipyards\n",
    "\n",
    "    shipyard_to_scores = defaultdict(list)\n",
    "    for p in board:\n",
    "        if p.kore > 50:\n",
    "            continue\n",
    "\n",
    "        closed_shipyard = None\n",
    "        min_distance = board.size\n",
    "        for shipyard in shipyards:\n",
    "            distance = shipyard.point.distance_from(p)\n",
    "            if shipyard.player_id != player.game_id:\n",
    "                distance -= 1\n",
    "\n",
    "            if distance < min_distance:\n",
    "                closed_shipyard = shipyard\n",
    "                min_distance = distance\n",
    "\n",
    "        if (\n",
    "            not closed_shipyard\n",
    "            or closed_shipyard.player_id != player.game_id\n",
    "            or min_distance < 3\n",
    "            or min_distance > 5\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        nearby_kore = sum(x.kore for x in p.nearby_points(10))\n",
    "        nearby_shipyards = sum(1 for x in board.shipyards if x.distance_from(p) < 5)\n",
    "        score = nearby_kore - 1000 * nearby_shipyards - 1000 * min_distance\n",
    "        shipyard_to_scores[closed_shipyard].append({\"score\": score, \"point\": p})\n",
    "\n",
    "    shipyard_to_point = {}\n",
    "    for shipyard, scores in shipyard_to_scores.items():\n",
    "        if scores:\n",
    "            scores = sorted(scores, key=lambda x: x[\"score\"])\n",
    "            point = scores[-1][\"point\"]\n",
    "            shipyard_to_point[shipyard] = point\n",
    "\n",
    "    return shipyard_to_point\n",
    "\n",
    "\n",
    "def need_more_shipyards(player: Player) -> int:\n",
    "    board = player.board\n",
    "\n",
    "    if player.ship_count < 100:\n",
    "        return 0\n",
    "\n",
    "    fleet_distance = []\n",
    "    for sy in player.shipyards:\n",
    "        for f in sy.incoming_allied_fleets:\n",
    "            fleet_distance.append(len(f.route))\n",
    "\n",
    "    if not fleet_distance:\n",
    "        return 0\n",
    "\n",
    "    mean_fleet_distance = sum(fleet_distance) / len(fleet_distance)\n",
    "\n",
    "    shipyard_production_capacity = sum(x.max_ships_to_spawn for x in player.shipyards)\n",
    "\n",
    "    steps_left = board.steps_left\n",
    "    if steps_left > 100:\n",
    "        scale = 3\n",
    "    elif steps_left > 50:\n",
    "        scale = 4\n",
    "    elif steps_left > 10:\n",
    "        scale = 100\n",
    "    else:\n",
    "        scale = 1000\n",
    "\n",
    "    needed = player.kore > scale * shipyard_production_capacity * mean_fleet_distance\n",
    "    if not needed:\n",
    "        return 0\n",
    "\n",
    "    current_shipyard_count = len(player.shipyards)\n",
    "\n",
    "    op_shipyard_positions = {\n",
    "        x.point for x in board.shipyards if x.player_id != player.game_id\n",
    "    }\n",
    "    expected_shipyard_count = current_shipyard_count + sum(\n",
    "        1\n",
    "        for x in player.fleets\n",
    "        if x.route.last_action() == Convert or x.route.end in op_shipyard_positions\n",
    "    )\n",
    "\n",
    "    opponent_shipyard_count = max(len(x.shipyards) for x in player.opponents)\n",
    "    opponent_ship_count = max(x.ship_count for x in player.opponents)\n",
    "    if (\n",
    "        expected_shipyard_count > opponent_shipyard_count\n",
    "        and player.ship_count < opponent_ship_count\n",
    "    ):\n",
    "        return 0\n",
    "\n",
    "    if current_shipyard_count < 10:\n",
    "        if expected_shipyard_count > current_shipyard_count:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    return max(0, 5 - (expected_shipyard_count - current_shipyard_count))\n",
    "\n",
    "\n",
    "def is_safety_route_to_convert(route_points: List[Point], player: Player):\n",
    "    board = player.board\n",
    "\n",
    "    target_point = route_points[-1]\n",
    "    target_time = len(route_points)\n",
    "    for pl in board.players:\n",
    "        if pl != player:\n",
    "            for t, positions in pl.expected_fleets_positions.items():\n",
    "                if t >= target_time and target_point in positions:\n",
    "                    return False\n",
    "\n",
    "    shipyard_positions = {x.point for x in board.shipyards}\n",
    "\n",
    "    for time, point in enumerate(route_points):\n",
    "        for pl in board.players:\n",
    "            if point in shipyard_positions:\n",
    "                return False\n",
    "\n",
    "            is_enemy = pl != player\n",
    "\n",
    "            if point in pl.expected_fleets_positions[time]:\n",
    "                return False\n",
    "\n",
    "            if is_enemy:\n",
    "                if point in pl.expected_dmg_positions[time]:\n",
    "                    return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "#%%writefile geometry.py\n",
    "\n",
    "# <--->\n",
    "\"\"\"\n",
    "from basic import Obj, cached_call, cached_property, min_ship_count_for_flight_plan_len\n",
    "\"\"\"\n",
    "# <--->\n",
    "\n",
    "\n",
    "###\n",
    "#%%writefile helpers.py\n",
    "\n",
    "\n",
    "# <--->\n",
    "\"\"\"\n",
    "from geometry import Point\n",
    "from board import Board, Player, BoardRoute, PlanRoute\n",
    "\"\"\"\n",
    "# <--->\n",
    "\n",
    "\n",
    "def is_intercept_route(\n",
    "    route: BoardRoute, player: Player, safety=True, allow_shipyard_intercept=False\n",
    "):\n",
    "    board = player.board\n",
    "\n",
    "    if not allow_shipyard_intercept:\n",
    "        shipyard_points = {x.point for x in board.shipyards}\n",
    "    else:\n",
    "        shipyard_points = {}\n",
    "\n",
    "    for time, point in enumerate(route.points()[:-1]):\n",
    "        if point in shipyard_points:\n",
    "            return True\n",
    "\n",
    "        for pl in board.players:\n",
    "            is_enemy = pl != player\n",
    "\n",
    "            if point in pl.expected_fleets_positions[time]:\n",
    "                return True\n",
    "\n",
    "            if safety and is_enemy:\n",
    "                if point in pl.expected_dmg_positions[time]:\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_shortcut_routes(\n",
    "    board: Board,\n",
    "    start: Point,\n",
    "    end: Point,\n",
    "    player: Player,\n",
    "    num_ships: int,\n",
    "    safety: bool = True,\n",
    "    allow_shipyard_intercept=False,\n",
    "    route_distance=None\n",
    ") -> List[BoardRoute]:\n",
    "    if route_distance is None:\n",
    "        route_distance = start.distance_from(end)\n",
    "    routes = []\n",
    "    for p in board:\n",
    "        distance = start.distance_from(p) + p.distance_from(end)\n",
    "        if distance != route_distance:\n",
    "            continue\n",
    "\n",
    "        path1 = start.dirs_to(p)\n",
    "        path2 = p.dirs_to(end)\n",
    "        random.shuffle(path1)\n",
    "        random.shuffle(path2)\n",
    "\n",
    "        plan = PlanRoute(path1 + path2)\n",
    "\n",
    "        if num_ships < plan.min_fleet_size():\n",
    "            continue\n",
    "\n",
    "        route = BoardRoute(start, plan)\n",
    "\n",
    "        if is_intercept_route(\n",
    "            route,\n",
    "            player,\n",
    "            safety=safety,\n",
    "            allow_shipyard_intercept=allow_shipyard_intercept,\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        routes.append(route)\n",
    "\n",
    "    return routes\n",
    "\n",
    "\n",
    "def is_invitable_victory(player: Player):\n",
    "    if not player.opponents:\n",
    "        return True\n",
    "\n",
    "    board = player.board\n",
    "    if board.steps_left > 100:\n",
    "        return False\n",
    "\n",
    "    board_kore = sum(x.kore for x in board) * (1 + board.regen_rate) ** board.steps_left\n",
    "\n",
    "    player_kore = player.kore + player.fleet_expected_kore()\n",
    "    opponent_kore = max(x.kore + x.fleet_expected_kore() for x in player.opponents)\n",
    "    return player_kore > opponent_kore + board_kore\n",
    "\n",
    "\n",
    "  ###\n",
    "#  %%writefile logger.py\n",
    "import os\n",
    "import logging\n",
    "\n",
    "FILE = \"game.log\"\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle_simulations\")\n",
    "LEVEL = logging.DEBUG if not IS_KAGGLE else logging.INFO\n",
    "LOGGING_ENABLED = False\n",
    "\n",
    "\n",
    "class _FileHandler(logging.FileHandler):\n",
    "    def emit(self, record):\n",
    "        if not LOGGING_ENABLED:\n",
    "            return\n",
    "\n",
    "        if IS_KAGGLE:\n",
    "            print(self.format(record))\n",
    "        else:\n",
    "            super().emit(record)\n",
    "\n",
    "\n",
    "def init_logger(_logger):\n",
    "    if not IS_KAGGLE:\n",
    "        if os.path.exists(FILE):\n",
    "            os.remove(FILE)\n",
    "\n",
    "    while _logger.hasHandlers():\n",
    "        _logger.removeHandler(_logger.handlers[0])\n",
    "\n",
    "    _logger.setLevel(LEVEL)\n",
    "    ch = _FileHandler(FILE)\n",
    "    ch.setLevel(LEVEL)\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%H-%M-%S\"\n",
    "    )\n",
    "    ch.setFormatter(formatter)\n",
    "    _logger.addHandler(ch)\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "#%%writefile main.py\n",
    "# <--->\n",
    "\"\"\"\n",
    "from board import Board\n",
    "from logger import logger, init_logger\n",
    "from offence import capture_shipyards\n",
    "from defence import defend_shipyards\n",
    "from expantion import expand\n",
    "from mining import mine\n",
    "from control import spawn, greedy_spawn, adjacent_attack, direct_attack\n",
    "\"\"\"\n",
    "# <--->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  %%writefile mining.py\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "# <--->\n",
    "\"\"\"\n",
    "from geometry import PlanRoute\n",
    "from board import Player, BoardRoute, Launch, Shipyard\n",
    "from helpers import is_intercept_route\n",
    "\"\"\"\n",
    "# <--->\n",
    "\n",
    "\n",
    "def mine(agent: Player):\n",
    "    board = agent.board\n",
    "    if not agent.opponents:\n",
    "        return\n",
    "\n",
    "    safety = False\n",
    "    my_ship_count = agent.ship_count\n",
    "    op_ship_count = max(x.ship_count for x in agent.opponents)\n",
    "    if my_ship_count < 2 * op_ship_count:\n",
    "        safety = True\n",
    "\n",
    "    op_ship_count = []\n",
    "    for op in agent.opponents:\n",
    "        for fleet in op.fleets:\n",
    "            op_ship_count.append(fleet.ship_count)\n",
    "\n",
    "    if not op_ship_count:\n",
    "        mean_fleet_size = 0\n",
    "        max_fleet_size = np.inf\n",
    "    else:\n",
    "        mean_fleet_size = np.percentile(op_ship_count, 75)\n",
    "        max_fleet_size = int(max(op_ship_count) * 1.1)\n",
    "\n",
    "    point_to_score = estimate_board_risk(agent)\n",
    "\n",
    "    shipyard_count = len(agent.shipyards)\n",
    "    if shipyard_count < 10:\n",
    "        max_distance = 15\n",
    "    elif shipyard_count < 20:\n",
    "        max_distance = 12\n",
    "    else:\n",
    "        max_distance = 8\n",
    "\n",
    "    max_distance = min(int(board.steps_left // 2), max_distance)\n",
    "\n",
    "    for sy in agent.shipyards:\n",
    "        if sy.action:\n",
    "            continue\n",
    "\n",
    "        free_ships = sy.available_ship_count\n",
    "\n",
    "        if free_ships <= 2:\n",
    "            continue\n",
    "\n",
    "        routes = find_shipyard_mining_routes(\n",
    "            sy, safety=safety, max_distance=max_distance\n",
    "        )\n",
    "\n",
    "        route_to_score = {}\n",
    "        for route in routes:\n",
    "            route_points = route.points()\n",
    "\n",
    "            if all(point_to_score[x] > 0 for x in route_points):\n",
    "                num_ships_to_launch = free_ships\n",
    "            else:\n",
    "                if free_ships < mean_fleet_size:\n",
    "                    continue\n",
    "                num_ships_to_launch = min(free_ships, max_fleet_size)\n",
    "\n",
    "            score = route.expected_kore(board, num_ships_to_launch) / len(route)\n",
    "            route_to_score[route] = score\n",
    "\n",
    "        if not route_to_score:\n",
    "            continue\n",
    "\n",
    "        routes = sorted(route_to_score, key=lambda x: -route_to_score[x])\n",
    "        for route in routes:\n",
    "            if all(point_to_score[x] >= 1 for x in route):\n",
    "                num_ships_to_launch = free_ships\n",
    "            else:\n",
    "                num_ships_to_launch = min(free_ships, 199)\n",
    "            if num_ships_to_launch < route.plan.min_fleet_size():\n",
    "                continue\n",
    "            else:\n",
    "                sy.action = Launch(num_ships_to_launch, route)\n",
    "                break\n",
    "\n",
    "\n",
    "def estimate_board_risk(player: Player):\n",
    "    board = player.board\n",
    "\n",
    "    shipyard_to_area = defaultdict(list)\n",
    "    for p in board:\n",
    "        closest_shipyard = None\n",
    "        min_distance = board.size\n",
    "        for sh in board.shipyards:\n",
    "            distance = sh.point.distance_from(p)\n",
    "            if distance < min_distance:\n",
    "                closest_shipyard = sh\n",
    "                min_distance = distance\n",
    "\n",
    "        shipyard_to_area[closest_shipyard].append(p)\n",
    "\n",
    "    point_to_score = {}\n",
    "    for sy, points in shipyard_to_area.items():\n",
    "        if sy.player_id == player.game_id:\n",
    "            for p in points:\n",
    "                point_to_score[p] = 1\n",
    "        else:\n",
    "            for p in points:\n",
    "                point_to_score[p] = -1\n",
    "\n",
    "    return point_to_score\n",
    "\n",
    "\n",
    "def find_shipyard_mining_routes(\n",
    "    sy: Shipyard, safety=True, max_distance: int = 15\n",
    ") -> List[BoardRoute]:\n",
    "    if max_distance < 1:\n",
    "        return []\n",
    "\n",
    "    departure = sy.point\n",
    "    player = sy.player\n",
    "\n",
    "    destinations = set()\n",
    "    for shipyard in sy.player.shipyards:\n",
    "        siege = sum(x.ship_count for x in shipyard.incoming_hostile_fleets)\n",
    "        if siege >= shipyard.ship_count:\n",
    "            continue\n",
    "        destinations.add(shipyard.point)\n",
    "\n",
    "    if not destinations:\n",
    "        return []\n",
    "\n",
    "    routes = []\n",
    "    for c in sy.point.nearby_points(max_distance):\n",
    "        if c == departure or c in destinations:\n",
    "            continue\n",
    "\n",
    "        paths = departure.dirs_to(c)\n",
    "        random.shuffle(paths)\n",
    "        plan = PlanRoute(paths)\n",
    "        destination = sorted(destinations, key=lambda x: c.distance_from(x))[0]\n",
    "        if destination == departure:\n",
    "            plan += plan.reverse()\n",
    "        else:\n",
    "            paths = c.dirs_to(destination)\n",
    "            random.shuffle(paths)\n",
    "            plan += PlanRoute(paths)\n",
    "\n",
    "        route = BoardRoute(departure, plan)\n",
    "\n",
    "        if is_intercept_route(route, player, safety):\n",
    "            continue\n",
    "\n",
    "        routes.append(BoardRoute(departure, plan))\n",
    "\n",
    "    return routes\n",
    "\n",
    "\n",
    "#  %%writefile offence.py\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# <--->\n",
    "\"\"\"\n",
    "from basic import max_ships_to_spawn\n",
    "from board import Player, Shipyard, Launch\n",
    "from helpers import find_shortcut_routes\n",
    "from logger import logger\n",
    "\"\"\"\n",
    "# <--->\n",
    "\n",
    "\n",
    "class _ShipyardTarget:\n",
    "    def __init__(self, shipyard: Shipyard):\n",
    "        self.shipyard = shipyard\n",
    "        self.point = shipyard.point\n",
    "        self.expected_profit = self._estimate_profit()\n",
    "        self.reinforcement_distance = self._get_reinforcement_distance()\n",
    "        self._future_ship_count = self._estimate_future_ship_count()\n",
    "        self.total_incoming_power = self._get_total_incoming_power()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Target {self.shipyard}\"\n",
    "\n",
    "    def estimate_shipyard_power(self, time):\n",
    "        return self._future_ship_count[time]\n",
    "\n",
    "    def _get_total_incoming_power(self):\n",
    "        return sum(x.ship_count for x in self.shipyard.incoming_allied_fleets)\n",
    "\n",
    "    def _get_reinforcement_distance(self):\n",
    "        incoming_allied_fleets = self.shipyard.incoming_allied_fleets\n",
    "        if not incoming_allied_fleets:\n",
    "            return np.inf\n",
    "        return min(x.eta for x in incoming_allied_fleets)\n",
    "\n",
    "    def _estimate_profit(self):\n",
    "        board = self.shipyard.board\n",
    "        spawn_cost = board.spawn_cost\n",
    "        profit = sum(\n",
    "            2 * x.expected_kore() - x.ship_count * spawn_cost\n",
    "            for x in self.shipyard.incoming_allied_fleets\n",
    "        )\n",
    "        profit += spawn_cost * board.shipyard_cost\n",
    "        return profit\n",
    "\n",
    "    def _estimate_future_ship_count(self):\n",
    "        shipyard = self.shipyard\n",
    "        player = shipyard.player\n",
    "        board = shipyard.board\n",
    "\n",
    "        time_to_fleet_kore = defaultdict(int)\n",
    "        for sh in player.shipyards:\n",
    "            for f in sh.incoming_allied_fleets:\n",
    "                time_to_fleet_kore[len(f.route)] += f.expected_kore()\n",
    "\n",
    "        shipyard_reinforcements = defaultdict(int)\n",
    "        for f in shipyard.incoming_allied_fleets:\n",
    "            shipyard_reinforcements[len(f.route)] += f.ship_count\n",
    "\n",
    "        spawn_cost = board.spawn_cost\n",
    "        player_kore = player.kore\n",
    "        ship_count = shipyard.ship_count\n",
    "        future_ship_count = [ship_count]\n",
    "        for t in range(1, board.size + 1):\n",
    "            ship_count += shipyard_reinforcements[t]\n",
    "            player_kore += time_to_fleet_kore[t]\n",
    "\n",
    "            can_spawn = max_ships_to_spawn(shipyard.turns_controlled + t)\n",
    "            spawn_count = min(int(player_kore // spawn_cost), can_spawn)\n",
    "            player_kore -= spawn_count * spawn_cost\n",
    "            ship_count += spawn_count\n",
    "            future_ship_count.append(ship_count)\n",
    "\n",
    "        return future_ship_count\n",
    "\n",
    "\n",
    "def capture_shipyards(agent: Player, max_attack_distance=10):\n",
    "    board = agent.board\n",
    "    # get agent's shipyard that can attend attacks\n",
    "    agent_shipyards = [\n",
    "        x for x in agent.shipyards if x.available_ship_count >= 3 and not x.action\n",
    "    ]\n",
    "    # if none then return\n",
    "    if not agent_shipyards:\n",
    "        return\n",
    "\n",
    "    # get possible targets\n",
    "    targets = []\n",
    "    for op_sy in board.shipyards:\n",
    "        if op_sy.player_id == agent.game_id or op_sy.incoming_hostile_fleets:\n",
    "            continue\n",
    "        target = _ShipyardTarget(op_sy)\n",
    "        # if target.expected_profit > 0:\n",
    "        targets.append(target)\n",
    "\n",
    "    if not targets:\n",
    "        return\n",
    "\n",
    "    for t in targets:\n",
    "        shipyards = sorted(\n",
    "            agent_shipyards, key=lambda x: t.point.distance_from(x.point)\n",
    "        )\n",
    "\n",
    "        for sy in shipyards:\n",
    "            if sy.action:\n",
    "                continue\n",
    "                \n",
    "            # if too far, don't attack\n",
    "            distance = sy.point.distance_from(t.point)\n",
    "            if distance > max_attack_distance:\n",
    "                continue\n",
    "\n",
    "            power = t.estimate_shipyard_power(distance)\n",
    "            \n",
    "            # if agent's shipyard too weak, don't attack\n",
    "            if sy.available_ship_count <= power:\n",
    "                continue\n",
    "\n",
    "            num_ships_to_launch = min(sy.available_ship_count, int(power * 1.2))\n",
    "\n",
    "            routes = find_shortcut_routes(\n",
    "                board,\n",
    "                sy.point,\n",
    "                t.point,\n",
    "                agent,\n",
    "                num_ships_to_launch,\n",
    "            )\n",
    "            if routes:\n",
    "                route = random.choice(routes)\n",
    "                logger.info(\n",
    "                    f\"Attack shipyard {sy.point}->{t.point}\"\n",
    "                )\n",
    "                sy.action = Launch(num_ships_to_launch, route)\n",
    "                break\n",
    "              \n",
    "#Agent wants to be the last function in the file\n",
    "def agent(obs, conf):\n",
    "    if obs[\"step\"] == 0:\n",
    "        pass # init_logger(logger)\n",
    "\n",
    "    board = Board(obs, conf)\n",
    "    step = board.step\n",
    "    my_id = obs[\"player\"]\n",
    "    remaining_time = obs[\"remainingOverageTime\"]\n",
    "    logger.info(f\"<step_{step + 1}>, remaining_time={remaining_time:.1f}\")\n",
    "\n",
    "    try:\n",
    "        a = board.get_player(my_id)\n",
    "    except KeyError:\n",
    "        return {}\n",
    "\n",
    "    if not a.opponents:\n",
    "        return {}\n",
    "    defend_shipyards(a)\n",
    "    capture_shipyards(a)\n",
    "    adjacent_attack(a)\n",
    "    direct_attack(a)\n",
    "    expand(a)\n",
    "    greedy_spawn(a)\n",
    "    mine(a)\n",
    "    spawn(a)\n",
    "\n",
    "    return a.actions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ff359",
   "metadata": {
    "papermill": {
     "duration": 0.048601,
     "end_time": "2022-06-04T05:01:59.608723",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.560122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reward utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35361323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:01:59.712478Z",
     "iopub.status.busy": "2022-06-04T05:01:59.712203Z",
     "iopub.status.idle": "2022-06-04T05:01:59.720536Z",
     "shell.execute_reply": "2022-06-04T05:01:59.719759Z"
    },
    "papermill": {
     "duration": 0.062552,
     "end_time": "2022-06-04T05:01:59.723082",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.660530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reward_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reward_utils.py\n",
    "from config import GAME_CONFIG, SHIP_COST, SHIPYARD_COST\n",
    "from kaggle_environments.envs.kore_fleets.helpers import Board\n",
    "import numpy as np\n",
    "from math import floor\n",
    "\n",
    "# Compute weight constants -- See get_board_value's docstring\n",
    "_max_steps = GAME_CONFIG['episodeSteps']\n",
    "_end_of_asset_value = floor(.5 * _max_steps)            # Assets = fleets & shipyards\n",
    "_weights_assets = np.linspace(start=1, stop=0, num=_end_of_asset_value)\n",
    "_weights_kore = np.linspace(start=0, stop=1, num=_end_of_asset_value)\n",
    "WEIGHTS_ASSETS = np.append(_weights_assets, np.zeros(_max_steps - _end_of_asset_value))\n",
    "WEIGHTS_KORE = np.append(_weights_kore, np.ones(_max_steps - _end_of_asset_value))\n",
    "WEIGHTS_MAX_SPAWN = {x: (x+3)/4 for x in range(1, 11)}  # Value multiplier of a shipyard as a function of its max spawn, = calculate a weight according to its max_spawn\n",
    "WEIGHTS_KORE_IN_FLEETS = WEIGHTS_KORE * WEIGHTS_ASSETS/2  # Always equal or smaller than either, almost always smaller, note: kore in fleets = cargo\n",
    "\n",
    "\n",
    "def get_board_value(board: Board) -> float:\n",
    "    \"\"\"Computes the board value for the current player.\n",
    "\n",
    "    The board value captures how are we currently performing, compared to the opponent. Each player's partial board\n",
    "    value assesses the player's situation, taking into account their current kore, ship count, shipyard count\n",
    "    (including their max spawn) and kore carried by fleets. We then define the board value as the difference between\n",
    "    player's partial board values.\n",
    "    Flight plans and the positioning of fleet and shipyards do not flow into the board value (yet).\n",
    "\n",
    "    To keep things simple, we'll take a weighted sum as the partial board value. We need weighting since\n",
    "    the importance of each item changes over time. We don't need to have the most kore at the beginning of the game,\n",
    "    but we do at the end. Ship count won't help us win games in the latter stages, but it is crucial in the beginning.\n",
    "    Fleets and shipyards will be accounted for proportionally to their kore cost.\n",
    "\n",
    "    For efficiency, the weight factors are pre-computed at module level. Here is the logic behind the weighting:\n",
    "    WEIGHTS_KORE: Applied to the player's kore count. Increases linearly from 0 to 1. It reaches one before\n",
    "        the maximum game length is reached.\n",
    "    WEIGHTS_ASSETS: Applied to fleets and shipyards. Decreases linearly from 1 to 0 and reaches zero before the maximum\n",
    "        length. It emphasizes the need of having ships over kore at the beginning of the game.\n",
    "    WEIGHTS_MAX_SPAWN: Shipyard value is multiplied by its max spawn. This captures the idea that long-held shipyards\n",
    "        are more valuable.\n",
    "    WEIGHTS_KORE_IN_FLEETS: Kore in fleets should be valued, too. But its value must be upper-bounded by WEIGHTS_KORE\n",
    "        (it can never be better to have kore in cargo than home) and it must decrease in time, since it doesn't\n",
    "        count towards the end kore count.\n",
    "\n",
    "    Args:\n",
    "        board: The board for which we want to compute the value.\n",
    "\n",
    "    Returns:\n",
    "        The value of the board.\n",
    "    \"\"\"\n",
    "    board_value = 0\n",
    "    if not board:\n",
    "        return board_value\n",
    "\n",
    "    # Get the weights as a function of the 'current' game step\n",
    "    step = board.step\n",
    "    weight_kore, weight_assets, weight_cargo = WEIGHTS_KORE[step], WEIGHTS_ASSETS[step], WEIGHTS_KORE_IN_FLEETS[step]\n",
    "\n",
    "    # Compute the partial board values\n",
    "    for player in board.players.values():\n",
    "        player_fleets, player_shipyards = list(player.fleets), list(player.shipyards)   # get player's stats from player object\n",
    "\n",
    "        value_kore = weight_kore * player.kore  # get score from kore\n",
    "\n",
    "        value_fleets = weight_assets * SHIP_COST * (        # get score from fleets(outside + inside shipyard), value = weight * total kore cost of the fleets\n",
    "                sum(fleet.ship_count for fleet in player_fleets)\n",
    "                + sum(shipyard.ship_count for shipyard in player_shipyards)\n",
    "        )\n",
    "\n",
    "        value_shipyards = weight_assets * SHIPYARD_COST * (     # get score from shipyards, value = weight * total kore cost of the fleets(with the coefficient of max_spawn)\n",
    "            sum(shipyard.max_spawn * WEIGHTS_MAX_SPAWN[shipyard.max_spawn] for shipyard in player_shipyards)\n",
    "        )\n",
    "\n",
    "        value_kore_in_cargo = weight_cargo * sum(fleet.kore for fleet in player_fleets)     # get score from cargo\n",
    "\n",
    "        # Add (or subtract) the partial values to the total board value. The current player is always us.\n",
    "        modifier = 1 if player.is_current_player else -1\n",
    "        board_value += modifier * (value_kore + value_fleets + value_shipyards + value_kore_in_cargo)\n",
    "\n",
    "    return board_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c810b2",
   "metadata": {
    "papermill": {
     "duration": 0.050857,
     "end_time": "2022-06-04T05:01:59.829608",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.778751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f3045c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:01:59.930267Z",
     "iopub.status.busy": "2022-06-04T05:01:59.929763Z",
     "iopub.status.idle": "2022-06-04T05:01:59.942409Z",
     "shell.execute_reply": "2022-06-04T05:01:59.941574Z"
    },
    "papermill": {
     "duration": 0.066428,
     "end_time": "2022-06-04T05:01:59.945132",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.878704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helper.py\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import math\n",
    "from math import floor\n",
    "import random\n",
    "from kaggle_environments.envs.kore_fleets.helpers import ShipyardAction, Board, Direction\n",
    "from typing import Union, Tuple, Dict, List, Generator\n",
    "from kaggle_environments.helpers import Point\n",
    "from config import (\n",
    "    GAME_CONFIG\n",
    ")\n",
    "from itertools import groupby\n",
    "\n",
    "# Miner\n",
    "def getNearestLargestKore(shipyardPos: Point, board: Board) -> Point:\n",
    "    #print(\"finding max kore where shipyardPos: \", shipyardPos)\n",
    "    me = board.current_player\n",
    "    size = GAME_CONFIG['size']\n",
    "    \n",
    "    max_kore = 0\n",
    "    max_kore_pos = shipyardPos\n",
    "    # Get largest kore pos\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            pos = Point(j, size - 1 - i)\n",
    "            curr_cell_kore = board.cells[pos].kore\n",
    "            if curr_cell_kore > max_kore:\n",
    "                max_kore = curr_cell_kore\n",
    "                max_kore_pos = pos\n",
    "    #print(\"max_kore_pos: \", max_kore_pos, \" with \", max_kore, \" kores\")\n",
    "    return max_kore_pos\n",
    "\n",
    "def getNearbyLargestKore(shipyardPos: Point, board: Board) -> Point:\n",
    "    MAX_DIS = 3\n",
    "    size = GAME_CONFIG['size']\n",
    " \n",
    "    max_kore = 0\n",
    "    max_kore_pos = shipyardPos\n",
    "    # Get nearby largest kore pos\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            pos = Point(j, size - 1 - i)\n",
    "            curr_cell_kore = board.cells[pos].kore\n",
    "            if shipyardPos.distance_to(pos, size) <= MAX_DIS and curr_cell_kore > max_kore:\n",
    "                max_kore = curr_cell_kore\n",
    "                max_kore_pos = pos\n",
    "    #print(\"nearby max_kore_pos: \", max_kore_pos, \" with \", max_kore, \" kores\")\n",
    "    return max_kore_pos\n",
    "    \n",
    "def getFlightPlan(shipyardPos: Point, targetPos: Point, num_ships: int, board: Board) -> str:\n",
    "    #print(\"shipyardPos: \", shipyardPos)\n",
    "    #print(\"targetPos: \", targetPos)\n",
    "    me = board.current_player\n",
    "    \n",
    "    dx = int(targetPos.x - shipyardPos.x)\n",
    "    dy = int(targetPos.y - shipyardPos.y)\n",
    "    #print(\"dx: \", dx, \" dy: \", dy)\n",
    "    \n",
    "    if(dx > 0 and abs(dx) >= 11):\n",
    "        dx = -(21-dx)\n",
    "    if(dy > 0 and abs(dy) >= 11):\n",
    "        dy = -(21-dy)    \n",
    "    if(dx < 0 and abs(dx) >= 11):\n",
    "        dx = 21+dx    \n",
    "    if(dy < 0 and abs(dy) >= 11):\n",
    "        dy = 21+dy\n",
    "    #print(\"tweaked dx: \", dx, \" dy: \", dy)\n",
    "    \n",
    "    rough_plan= \"\"\n",
    "    if dx > 0 and dy > 0:\n",
    "        rough_plan = rough_plan + \"E\" * dx\n",
    "        rough_plan = rough_plan + \"N\" * dy\n",
    "    elif dx > 0 and dy < 0:\n",
    "        rough_plan = rough_plan + \"E\" * dx\n",
    "        rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dx < 0 and dy > 0:\n",
    "        rough_plan = rough_plan + \"W\" * -dx\n",
    "        rough_plan = rough_plan + \"N\" * dy\n",
    "    elif dx < 0 and dy < 0:\n",
    "        rough_plan = rough_plan + \"W\" * -dx\n",
    "        rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dx == 0:\n",
    "        if dy > 0:\n",
    "            rough_plan = rough_plan + \"N\" * dy\n",
    "        elif dy < 0:\n",
    "            rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dy == 0:\n",
    "        if dx > 0:\n",
    "            rough_plan = rough_plan + \"E\" * dx\n",
    "        elif dx < 0:\n",
    "            rough_plan = rough_plan + \"W\" * -dx\n",
    "\n",
    "    return simplify(rough_plan) + simplify(reversed_str(rough_plan))\n",
    "    \n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter+=1\n",
    "        shuffled = ''.join(random.sample(rough_plan,len(rough_plan)))\n",
    "        \n",
    "        flight_plan = simplify(shuffled)\n",
    "        \n",
    "        # len matches\n",
    "        if(len(flight_plan) < max_flight_plan_len(num_ships)):\n",
    "            return flight_plan + simplify(reversed_str(rough_plan))\n",
    "        # too many times\n",
    "        elif counter >=10:\n",
    "            return simplify(rough_plan) + simplify(reversed_str(rough_plan))\n",
    "        \n",
    "def max_flight_plan_len(num_ships):\n",
    "    return floor(2 * np.log(num_ships)) + 1\n",
    "    \n",
    "def simplify(plan: str) -> str:\n",
    "    #print(\"simplify input: \", plan)\n",
    "    groups = groupby(plan)\n",
    "    result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "    simplified = \"\".join(\"{}{}\".format(label, count-1) for label, count in result)\n",
    "    simplified = simplified.replace('0', '')\n",
    "    #print(\"simplified: \", simplified)\n",
    "    return simplified\n",
    "\n",
    "def reversed_str(original: str) -> str:\n",
    "    #print(\"original: \", original)\n",
    "    reversed_str = original[::-1]\n",
    "    return_str = \"\"\n",
    "    for c in reversed_str:\n",
    "        if(c == \"N\"):\n",
    "            return_str = return_str + \"S\"\n",
    "        elif(c == \"S\"):\n",
    "            return_str = return_str + \"N\"\n",
    "        elif(c == \"W\"):\n",
    "            return_str = return_str + \"E\"\n",
    "        elif(c == \"E\"):\n",
    "            return_str = return_str + \"W\"\n",
    "    #print(\"reversed string: \", return_str)\n",
    "    return return_str   \n",
    "\n",
    "# Attacker\n",
    "def getWeakestShipyard(shipyardPos: Point, board: Board) -> Point:\n",
    "    #print(\"finding max kore where shipyardPos: \", shipyardPos)\n",
    "    me = board.current_player\n",
    "    size = GAME_CONFIG['size']\n",
    "    \n",
    "    min_fleet = 5000\n",
    "    min_fleet_pos = shipyardPos\n",
    "    # Get weakest shipyard pos\n",
    "    for op in board.opponents:\n",
    "        for sy in op.shipyards:\n",
    "            pos = sy.position\n",
    "            curr_sy_fleet = sy.ship_count\n",
    "            if curr_sy_fleet < min_fleet:\n",
    "                min_fleet = curr_sy_fleet\n",
    "                min_fleet_pos = pos\n",
    "    return min_fleet_pos\n",
    "\n",
    "def getAttackFlightPlan(shipyardPos: Point, targetPos: Point, num_ships: int, board: Board) -> str:\n",
    "    #print(\"shipyardPos: \", shipyardPos)\n",
    "    #print(\"targetPos: \", targetPos)\n",
    "    me = board.current_player\n",
    "    \n",
    "    dx = int(targetPos.x - shipyardPos.x)\n",
    "    dy = int(targetPos.y - shipyardPos.y)\n",
    "    #print(\"dx: \", dx, \" dy: \", dy)\n",
    "    \n",
    "    if(dx > 0 and abs(dx) >= 11):\n",
    "        dx = -(21-dx)\n",
    "    if(dy > 0 and abs(dy) >= 11):\n",
    "        dy = -(21-dy)    \n",
    "    if(dx < 0 and abs(dx) >= 11):\n",
    "        dx = 21+dx    \n",
    "    if(dy < 0 and abs(dy) >= 11):\n",
    "        dy = 21+dy\n",
    "    #print(\"tweaked dx: \", dx, \" dy: \", dy)\n",
    "    \n",
    "    rough_plan= \"\"\n",
    "    if dx > 0 and dy > 0:\n",
    "        rough_plan = rough_plan + \"E\" * dx\n",
    "        rough_plan = rough_plan + \"N\" * dy\n",
    "    elif dx > 0 and dy < 0:\n",
    "        rough_plan = rough_plan + \"E\" * dx\n",
    "        rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dx < 0 and dy > 0:\n",
    "        rough_plan = rough_plan + \"W\" * -dx\n",
    "        rough_plan = rough_plan + \"N\" * dy\n",
    "    elif dx < 0 and dy < 0:\n",
    "        rough_plan = rough_plan + \"W\" * -dx\n",
    "        rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dx == 0:\n",
    "        if dy > 0:\n",
    "            rough_plan = rough_plan + \"N\" * dy\n",
    "        elif dy < 0:\n",
    "            rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dy == 0:\n",
    "        if dx > 0:\n",
    "            rough_plan = rough_plan + \"E\" * dx\n",
    "        elif dx < 0:\n",
    "            rough_plan = rough_plan + \"W\" * -dx\n",
    "\n",
    "    return simplify(rough_plan)\n",
    "    \n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter+=1\n",
    "        shuffled = ''.join(random.sample(rough_plan,len(rough_plan)))\n",
    "        \n",
    "        flight_plan = simplify(shuffled)\n",
    "        \n",
    "        # len matches\n",
    "        if(len(flight_plan) < max_flight_plan_len(num_ships)):\n",
    "            return flight_plan + simplify(reversed_str(rough_plan))\n",
    "        # too many times\n",
    "        elif counter >=10:\n",
    "            return simplify(rough_plan) + simplify(reversed_str(rough_plan))\n",
    "        \n",
    "# Builder\n",
    "def getBuildFlightPlan(shipyardPos: Point, targetPos: Point, num_ships: int, board: Board) -> str:\n",
    "    me = board.current_player\n",
    "    \n",
    "    dx = int(targetPos.x - shipyardPos.x)\n",
    "    dy = int(targetPos.y - shipyardPos.y)\n",
    "    \n",
    "    if(dx > 0 and abs(dx) >= 11):\n",
    "        dx = -(21-dx)\n",
    "    if(dy > 0 and abs(dy) >= 11):\n",
    "        dy = -(21-dy)    \n",
    "    if(dx < 0 and abs(dx) >= 11):\n",
    "        dx = 21+dx    \n",
    "    if(dy < 0 and abs(dy) >= 11):\n",
    "        dy = 21+dy\n",
    "    \n",
    "    rough_plan= \"\"\n",
    "    if dx > 0 and dy > 0:\n",
    "        rough_plan = rough_plan + \"E\" * dx\n",
    "        rough_plan = rough_plan + \"N\" * dy\n",
    "    elif dx > 0 and dy < 0:\n",
    "        rough_plan = rough_plan + \"E\" * dx\n",
    "        rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dx < 0 and dy > 0:\n",
    "        rough_plan = rough_plan + \"W\" * -dx\n",
    "        rough_plan = rough_plan + \"N\" * dy\n",
    "    elif dx < 0 and dy < 0:\n",
    "        rough_plan = rough_plan + \"W\" * -dx\n",
    "        rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dx == 0:\n",
    "        if dy > 0:\n",
    "            rough_plan = rough_plan + \"N\" * dy\n",
    "        elif dy < 0:\n",
    "            rough_plan = rough_plan + \"S\" * -dy\n",
    "    elif dy == 0:\n",
    "        if dx > 0:\n",
    "            rough_plan = rough_plan + \"E\" * dx\n",
    "        elif dx < 0:\n",
    "            rough_plan = rough_plan + \"W\" * -dx\n",
    "\n",
    "    return simplify(rough_plan) + \"C\"\n",
    "    \n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter+=1\n",
    "        shuffled = ''.join(random.sample(rough_plan,len(rough_plan)))\n",
    "        \n",
    "        flight_plan = simplify(shuffled)\n",
    "        \n",
    "        # len matches\n",
    "        if(len(flight_plan) < max_flight_plan_len(num_ships)):\n",
    "            return flight_plan + simplify(reversed_str(rough_plan))\n",
    "        # too many times\n",
    "        elif counter >=10:\n",
    "            return simplify(rough_plan) + simplify(reversed_str(rough_plan))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c5469",
   "metadata": {
    "papermill": {
     "duration": 0.049441,
     "end_time": "2022-06-04T05:02:00.045172",
     "exception": false,
     "start_time": "2022-06-04T05:01:59.995731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The KoreGymEnv wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1595bfb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:02:00.161999Z",
     "iopub.status.busy": "2022-06-04T05:02:00.161730Z",
     "iopub.status.idle": "2022-06-04T05:02:00.185358Z",
     "shell.execute_reply": "2022-06-04T05:02:00.184514Z"
    },
    "papermill": {
     "duration": 0.07971,
     "end_time": "2022-06-04T05:02:00.189040",
     "exception": false,
     "start_time": "2022-06-04T05:02:00.109330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing environment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile environment.py\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import math\n",
    "import random\n",
    "from typing import List\n",
    "from math import floor\n",
    "from kaggle_environments import make\n",
    "from kaggle_environments.envs.kore_fleets.helpers import ShipyardAction, Board, Direction\n",
    "from typing import Union, Tuple, Dict\n",
    "from reward_utils import get_board_value\n",
    "from helper import *\n",
    "from config import *\n",
    "\n",
    "class FlightPlan:\n",
    "    # possible tokens in flight plan:\n",
    "    #   0~9: #duplicates for the following token\n",
    "    #   N, E, S, W: directions\n",
    "    #   C: convert to shipyard\n",
    "\n",
    "    # convert plan from gym format (array)\n",
    "    # to that of kore (string)\n",
    "    def arr_to_str(self, plan_in_arr: list) -> str:\n",
    "        assert isinstance(plan_in_arr, np.ndarray)\n",
    "\n",
    "        def num_to_token(num) -> str:\n",
    "            assert isinstance(num, int)\n",
    "            \n",
    "            return (\n",
    "                \"N\" if num == 1 else\n",
    "                \"E\" if num == 2 else\n",
    "                \"S\" if num == 3 else\n",
    "                \"W\" if num == 4 else\n",
    "                \"C\" if num == 5 else\n",
    "                \"\"\n",
    "            )\n",
    "\n",
    "        plan_in_str = \"\"\n",
    "        for i, num in enumerate(plan_in_arr):\n",
    "            # int in [0, 5] \n",
    "            # 0 indicates end of flight plan\n",
    "            # the rest correspond to N, E, S, W, C\n",
    "            \n",
    "            # plan must start with \"NESW\"\n",
    "            if i == 0: \n",
    "                low_out = 1\n",
    "                high_out = 4 + .99 # to ensure equal likelyhood\n",
    "            else:\n",
    "                low_out = 0\n",
    "                high_out = 5 + .99\n",
    "\n",
    "            num = int(np.clip(num, 0, 1) * (high_out - low_out) + low_out) \n",
    "\n",
    "            token = num_to_token(num)\n",
    "            if not token:\n",
    "                break\n",
    "\n",
    "            plan_in_str += token\n",
    "\n",
    "        # flight plan here is not truncated\n",
    "        # e.g. 3E is represented as EEE\n",
    "        return self._truncate_flight_plan(plan_in_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _truncate_flight_plan(flight_plan: str) -> str:\n",
    "        assert isinstance(flight_plan, str)\n",
    "\n",
    "        if len(flight_plan) <= 1:\n",
    "            return flight_plan\n",
    "\n",
    "        # flight plan must start with NESW\n",
    "        # do not truncate the 1st token\n",
    "        fp = flight_plan[0]\n",
    "        flight_plan = flight_plan[1:]\n",
    "\n",
    "        prev_token = flight_plan[0]\n",
    "        duplicate_cnt = 1\n",
    "        for token in flight_plan[1:]:\n",
    "            if token != prev_token:\n",
    "                if duplicate_cnt > 1:\n",
    "                    fp += str(duplicate_cnt)\n",
    "                \n",
    "                fp += prev_token\n",
    "                duplicate_cnt = 1\n",
    "            else:\n",
    "                duplicate_cnt += 1\n",
    "\n",
    "            prev_token = token\n",
    "\n",
    "        if duplicate_cnt > 1:\n",
    "            fp += str(duplicate_cnt)\n",
    "        \n",
    "        fp += prev_token\n",
    "\n",
    "        return fp\n",
    "\n",
    "    # convert plan from kore format (string)\n",
    "    # to that of gym (array)\n",
    "    def str_to_arr(self, plan_in_str: str) -> list:\n",
    "        def token_to_num(token: str) -> int:\n",
    "            assert isinstance(token, str)\n",
    "\n",
    "            return (\n",
    "                1 if token == \"N\" else\n",
    "                2 if token == \"E\" else\n",
    "                3 if token == \"S\" else\n",
    "                4 if token == \"W\" else\n",
    "                5 if token == \"C\" else\n",
    "                0\n",
    "            )\n",
    "\n",
    "        expanded_fp = self._expand_flight_plan(plan_in_str)\n",
    "        plan_in_array = np.zeros(MAX_FP_LEN)\n",
    "\n",
    "        for i, token in enumerate(expanded_fp):\n",
    "            if i >= MAX_FP_LEN:\n",
    "                break\n",
    "            \n",
    "            plan_in_array[i] = token_to_num(token)\n",
    "\n",
    "        return plan_in_array\n",
    "    \n",
    "    @staticmethod\n",
    "    def _expand_flight_plan(flight_plan: str) -> str:\n",
    "        assert isinstance(flight_plan, str)\n",
    "\n",
    "        # 3W2E -> 000WWE\n",
    "        #print(\"original: \", flight_plan)\n",
    "        fp = \"\"        \n",
    "        while flight_plan:\n",
    "            i = 0\n",
    "            while i < len(flight_plan) and flight_plan[i].isnumeric():\n",
    "                i += 1\n",
    "\n",
    "            if i == len(flight_plan):\n",
    "                # plan ends with a number\n",
    "                # but trailing number has no effect\n",
    "                break\n",
    "            elif i == 0:\n",
    "                # plan starts with NESWC\n",
    "                fp += flight_plan[i]\n",
    "            else:\n",
    "                # plan starts with a number\n",
    "                fp += int(flight_plan[:i]) * \"0\" + flight_plan[i]\n",
    "\n",
    "            flight_plan = flight_plan[i+1:]\n",
    "        #print(\"after: \", fp)\n",
    "        return fp\n",
    "\n",
    "class KoreGymEnv(gym.Env):\n",
    "    \"\"\"An openAI-gym env wrapper for kaggle's kore environment. Can be used with stable-baselines3.\n",
    "\n",
    "    There are three fundamental components to this class which you would want to customize for your own agents:\n",
    "        The action space is defined by `action_space` and `gym_to_kore_action()`\n",
    "        The state space (observations) is defined by `state_space` and `obs_as_gym_state()`\n",
    "        The reward is computed with `compute_reward()`\n",
    "\n",
    "    Note that the action and state spaces define the inputs and outputs to your model *as numpy arrays*. Use the\n",
    "    functions mentioned above to translate these arrays into actual kore environment observations and actions.\n",
    "\n",
    "    The rest is basically boilerplate and makes sure that the kaggle environment plays nicely with stable-baselines3.\n",
    "\n",
    "    Usage:\n",
    "        >>> from stable_baselines3 import PPO\n",
    "        >>>\n",
    "        >>> kore_env = KoreGymEnv()\n",
    "        >>> model = PPO('MlpPolicy', kore_env, verbose=1)\n",
    "        >>> model.learn(total_timesteps=100000)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, agents=None, debug=None):\n",
    "        super(KoreGymEnv, self).__init__()\n",
    "\n",
    "        if not config:\n",
    "            config = GAME_CONFIG\n",
    "        if not agents:\n",
    "            agents = GAME_AGENTS\n",
    "        if not debug:\n",
    "            debug = True\n",
    "\n",
    "        self.agents = agents\n",
    "        self.env = make(\"kore_fleets\", configuration=config, debug=debug)\n",
    "        self.config = self.env.configuration\n",
    "        self.trainer = None\n",
    "        self.raw_obs = None\n",
    "        self.previous_obs = None\n",
    "\n",
    "        # Define the action and state space\n",
    "        # Change these to match your needs. Normalization to the [-1, 1] interval is recommended. See:\n",
    "        # https://araffin.github.io/slides/rlvs-tips-tricks/#/13/0/0\n",
    "        # See https://www.gymlibrary.ml/content/spaces/ for more info on OpenAI-gym spaces.\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=ACTION_SIZE,\n",
    "            dtype=DTYPE\n",
    "        )\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=OBSERVATION_SIZE,\n",
    "            dtype=DTYPE\n",
    "        )\n",
    "\n",
    "        self.strict_reward = config.get('strict', False)    # is it strict or not, used when evaluating trained agent\n",
    "\n",
    "        # Debugging info - Enable or disable as needed\n",
    "        self.reward = 0\n",
    "        self.n_steps = 0\n",
    "        self.n_resets = 0\n",
    "        self.n_dones = 0\n",
    "        self.last_action = None\n",
    "        self.last_done = False\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Resets the trainer and returns the initial observation in state space. Used when training & evaluting\n",
    "\n",
    "        Returns:\n",
    "            self.obs_as_gym_state: the current observation encoded as a state in state space\n",
    "        \"\"\"\n",
    "        # agents = self.agents if np.random.rand() > .5 else self.agents[::-1]  # Randomize starting position\n",
    "        self.trainer = self.env.train(self.agents)\n",
    "        self.raw_obs = self.trainer.reset()\n",
    "        self.n_resets += 1\n",
    "        return self.obs_as_gym_state\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"Execute action in the trainer and return the results.\n",
    "\n",
    "        Args:\n",
    "            action: The action in action space, i.e. the output of the stable-baselines3 agent\n",
    "\n",
    "        Returns:\n",
    "            self.obs_as_gym_state: the current observation encoded as a state in state space\n",
    "            reward: The agent's reward\n",
    "            done: If True, the episode is over\n",
    "            info: A dictionary with additional debugging information\n",
    "        \"\"\"\n",
    "        kore_action = self.gym_to_kore_action(action)\n",
    "        self.previous_obs = self.raw_obs\n",
    "        self.raw_obs, _, done, info = self.trainer.step(kore_action)  # Ignore trainer reward, which is just delta kore\n",
    "        self.reward = self.compute_reward(done)\n",
    "\n",
    "        # Debugging info\n",
    "        # with open('logs/tmp.log', 'a') as log:\n",
    "        #    print(kore_action.action_type, kore_action.num_ships, kore_action.flight_plan, file=log)\n",
    "        #    if done:\n",
    "        #        print('done', file=log)\n",
    "        #    if info:\n",
    "        #        print('info', file=log)\n",
    "        self.n_steps += 1\n",
    "        self.last_done = done\n",
    "        self.last_action = kore_action\n",
    "        self.n_dones += 1 if done else 0\n",
    "\n",
    "        return self.obs_as_gym_state, self.reward, done, info\n",
    "\n",
    "    def render(self, **kwargs):\n",
    "        self.env.render(**kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def board(self):\n",
    "        return Board(self.raw_obs, self.config)\n",
    "\n",
    "    @property\n",
    "    def previous_board(self):\n",
    "        return Board(self.previous_obs, self.config)\n",
    "    \n",
    "\n",
    "    def gym_to_kore_action(self, gym_action: np.ndarray) -> Dict[str, str]:\n",
    "        \"\"\"Decode an action in action space as a kore action.\n",
    "\n",
    "        In other words, transform a stable-baselines3 action into an action compatible with the kore environment.\n",
    "\n",
    "        This method is central - It defines how the agent output is mapped to kore actions.\n",
    "        You can modify it to suit your needs.\n",
    "\n",
    "        Our gym_action is a 1-dimensional vector of size 2 (as defined in self.action_space). \n",
    "        We will interpret the values as follows:\n",
    "        gym_action[0] represents the identity of the launched fleet or for shipyards to build ships\n",
    "        gym_action[0]:\n",
    "        - -1 ~ -0.6: shipyard defender\n",
    "        - -0.6 ~ -0.2: attacker(include fleets / shipyards)\n",
    "        - -0.2 ~ 0.2: shipyard builder\n",
    "        - 0.2 ~ 0.6: greedy spawner\n",
    "        - 0.6 ~ 1: miner\n",
    "        abs(gym_action[1]) encodes the number of ships to build/launch.\n",
    "        gym_action[2] the target to go (x axis)\n",
    "        gym_action[3] the target to go (y axis)\n",
    "\n",
    "        Notes: The same action is sent to all shipyards, though we make sure that the actions are valid.\n",
    "\n",
    "        Args:\n",
    "            gym_action: The action produces by our stable-baselines3 agent.\n",
    "\n",
    "        Returns:\n",
    "            The corresponding kore environment actions or None if the agent wants to wait.\n",
    "\n",
    "        \"\"\"         \n",
    "        action_launch = gym_action[0] > 0\n",
    "        action_build = gym_action[0] < 0\n",
    "        # Mapping the number of ships is an interesting exercise. Here we chose a linear mapping to the interval\n",
    "        # [1, MAX_ACTION_FLEET_SIZE], but you could use something else. With a linear mapping, all values are\n",
    "        # evenly spaced. An exponential mapping, however, would space out lower values, making them easier for the agent\n",
    "        # to distinguish and choose, at the cost of needing more precision to accurately select higher values.\n",
    "        number_of_ships = int(\n",
    "            clip_normalize(\n",
    "                x=abs(gym_action[1]),\n",
    "                low_in=0,\n",
    "                high_in=1,\n",
    "                low_out=1,\n",
    "                high_out=MAX_ACTION_FLEET_SIZE\n",
    "            )\n",
    "        )\n",
    "        gym_action[2] = int(\n",
    "            clip_normalize(\n",
    "                x=gym_action[2],\n",
    "                low_in=-1,\n",
    "                high_in=1,\n",
    "                low_out=0,\n",
    "                high_out=GAME_CONFIG['size']-1\n",
    "            )\n",
    "        )\n",
    "        gym_action[3] = int(\n",
    "            clip_normalize(\n",
    "                x=gym_action[3],\n",
    "                low_in=-1,\n",
    "                high_in=1,\n",
    "                low_out=0,\n",
    "                high_out=GAME_CONFIG['size']-1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Broadcast the same action to all shipyards\n",
    "        board = self.board\n",
    "        me = board.current_player\n",
    "        for shipyard in me.shipyards:\n",
    "            action = None\n",
    "            # Shipyard defenser, note: now does the same as greedy spawner, should solve the shipyard problem first\n",
    "            if -1 <= gym_action[0] < -0.6:\n",
    "                # Limit the number of ships to the maximum that can be actually built\n",
    "                max_spawn = shipyard.max_spawn\n",
    "                max_purchasable = floor(me.kore / self.config[\"spawnCost\"])\n",
    "                number_of_ships = min(number_of_ships, max_spawn, max_purchasable)\n",
    "                if number_of_ships:\n",
    "                    action = ShipyardAction.spawn_ships(number_ships=number_of_ships)\n",
    "                #print(\"Defense \", number_of_ships)\n",
    "                \n",
    "            # Attacker\n",
    "            if -0.6 <= gym_action[0] < -0.2:\n",
    "                # Limit the number of ships to the amount that is actually present in the shipyard\n",
    "                shipyard_count = shipyard.ship_count\n",
    "                number_of_ships = min(number_of_ships, floor(shipyard_count * 2 / 3)) # *2/3 for not sending every fleet out\n",
    "                \n",
    "                # Decide where to attack\n",
    "                if number_of_ships:\n",
    "                    target_pos = Point(gym_action[2], gym_action[3])\n",
    "                    flight_plan = getAttackFlightPlan(shipyard.position, target_pos, number_of_ships, self.board)\n",
    "                    # if flight plan too long, go attack weakest shipyard\n",
    "                    if(len(flight_plan) > max_flight_plan_len(number_of_ships)):\n",
    "                        target_pos = getWeakestShipyard(shipyard.position, self.board)\n",
    "                        flight_plan = getAttackFlightPlan(shipyard.position, target_pos, number_of_ships, self.board)\n",
    "                    # if flight plan empty or still too long, random choose a direction\n",
    "                    if not flight_plan or len(flight_plan) > max_flight_plan_len(number_of_ships):\n",
    "                        action = ShipyardAction.launch_fleet_in_direction(number_ships=number_of_ships,\n",
    "                                                                          direction=Direction.random_direction())\n",
    "                    # launch flight plan if nonempty\n",
    "                    else:\n",
    "                        action = ShipyardAction.launch_fleet_with_flight_plan(number_ships=number_of_ships, \n",
    "                                                                              flight_plan=flight_plan)\n",
    "                    #print(\"Attack \", target_pos[0], target_pos[1], \" with \", number_of_ships)\n",
    "                    #print(flight_plan)\n",
    "            # Builder\n",
    "            elif -0.2 <= gym_action[0] < 0.2:\n",
    "                # Limit the number of ships to the amount that is actually present in the shipyard\n",
    "                shipyard_count = shipyard.ship_count\n",
    "                number_of_ships = min(number_of_ships, floor(shipyard_count * 2 / 3)) # *2/3 for not sending every fleet out\n",
    "                \n",
    "                # Get flight plan\n",
    "                if number_of_ships >= 50:\n",
    "                    target_pos = Point(gym_action[2], gym_action[3])\n",
    "                    flight_plan = getBuildFlightPlan(shipyard.position, target_pos, number_of_ships, self.board)\n",
    "                    # if flight plan too long, do greedy (since normal mine is bound to have longer flight plan)\n",
    "                    if(len(flight_plan) > max_flight_plan_len(number_of_ships)):\n",
    "                        target_pos = getNearbyLargestKore(shipyard.position, self.board)\n",
    "                        flight_plan = getFlightPlan(shipyard.position, target_pos, number_of_ships, self.board)\n",
    "                    # if flight plan empty or still too long, random choose a direction\n",
    "                    if not flight_plan or len(flight_plan) > max_flight_plan_len(number_of_ships):\n",
    "                        action = ShipyardAction.launch_fleet_in_direction(number_ships=number_of_ships,\n",
    "                                                                          direction=Direction.random_direction())\n",
    "                    # launch flight plan if nonempty\n",
    "                    else:\n",
    "                        action = ShipyardAction.launch_fleet_with_flight_plan(number_ships=number_of_ships, \n",
    "                                                                              flight_plan=flight_plan)\n",
    "                    #print(\"Build \", target_pos[0], target_pos[1], \" with \", number_of_ships)\n",
    "                    #print(flight_plan)\n",
    "            # Greedy Spawner\n",
    "            elif 0.2 <= gym_action[0] < 0.6:\n",
    "                # Limit the number of ships to the maximum that can be actually built\n",
    "                max_spawn = shipyard.max_spawn\n",
    "                max_purchasable = floor(me.kore / self.config[\"spawnCost\"])\n",
    "                number_of_ships = min(number_of_ships, max_spawn, max_purchasable)\n",
    "                if number_of_ships:\n",
    "                    action = ShipyardAction.spawn_ships(number_ships=number_of_ships)\n",
    "                #print(\"Spawn \", number_of_ships)\n",
    "            # Miner\n",
    "            elif 0.6 <= gym_action[0] <= 1:\n",
    "                # Get number of ships to launch\n",
    "                shipyard_count = shipyard.ship_count\n",
    "                number_of_ships = min(number_of_ships, floor(shipyard_count * 2 / 3)) # *2/3 for not sending every fleet out\n",
    "                if number_of_ships:\n",
    "                    target_pos = Point(gym_action[2], gym_action[3])\n",
    "                    flight_plan = getFlightPlan(shipyard.position, target_pos, number_of_ships, self.board)\n",
    "                    #print(gym_action[2], gym_action[3])\n",
    "                    # if flight plan too long, go get max kore\n",
    "                    if(len(flight_plan) > max_flight_plan_len(number_of_ships)):\n",
    "                        target_pos = getNearestLargestKore(shipyard.position, self.board)\n",
    "                        flight_plan = getFlightPlan(shipyard.position, target_pos, number_of_ships, self.board)\n",
    "                        #print(\"######### do go fetch max kore, flight plan: \", flight_plan) \n",
    "                    # flight plan still too long, do greedy mine\n",
    "                    if(len(flight_plan) > max_flight_plan_len(number_of_ships)):\n",
    "                        target_pos = getNearbyLargestKore(shipyard.position, self.board)\n",
    "                        flight_plan = getFlightPlan(shipyard.position, target_pos, number_of_ships, self.board)\n",
    "                        #print(\"######### do greedy, flight plan: \", flight_plan) \n",
    "                    # if flight plan empty or still too long, random choose a direction\n",
    "                    if not flight_plan or len(flight_plan) > max_flight_plan_len(number_of_ships):\n",
    "                        #print(\"######### random flight plan: \", flight_plan)\n",
    "                        action = ShipyardAction.launch_fleet_in_direction(number_ships=number_of_ships,\n",
    "                                                                          direction=Direction.random_direction())\n",
    "                    # launch flight plan if nonempty\n",
    "                    else:\n",
    "                        #print(\"######### launch miner flight plan: \", flight_plan)\n",
    "                        action = ShipyardAction.launch_fleet_with_flight_plan(number_ships=number_of_ships, \n",
    "                                                                              flight_plan=flight_plan)\n",
    "                    #print(\"Mine \", target_pos[0], target_pos[1], \" with \", number_of_ships)\n",
    "                    #print(flight_plan)\n",
    "            shipyard.next_action = action\n",
    "\n",
    "        return me.next_actions\n",
    "\n",
    "    @property\n",
    "    def obs_as_gym_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return the current observation encoded as a state in state space.\n",
    "        #################### 2D features ####################\n",
    "        ######### 21x21x4 (size x size x n_features) ########\n",
    "        Cell:\n",
    "            feat 0. #kore in the cell\n",
    "        Shipyard:\n",
    "            feat 1. #ships (*= -1 if the yard belongs to enemy)\n",
    "            feat 2. max spawn\n",
    "        Fleet:\n",
    "            feat 3. #ships (*= -1 if the fleet belongs to enemy)\n",
    "            feat 4. current direction\n",
    "            \n",
    "        #################### 3D features ####################\n",
    "        ### 21x21x5 (size x size x MAX_OBSERVABLE_FP_LEN) ###\n",
    "        Fleet:\n",
    "            feat 5. flight plan\n",
    "        #################### 1D features ####################\n",
    "        ################### N_1D_FEATURES ###################\n",
    "        General:\n",
    "            feat 6: #plays so far\n",
    "            feat 7: #kore I have\n",
    "            feat 8: #kore the opponent has\n",
    "        \"\"\"\n",
    "\n",
    "        state_2D = np.zeros(shape=(self.config.size, self.config.size, N_2D_FEATURES + MAX_FP_LEN))\n",
    "        board = self.board\n",
    "        my_id = board.current_player_id\n",
    "\n",
    "        for point, cell in board.cells.items():\n",
    "            # feat 0: #kore in the cell\n",
    "            state_2D[point.y, point.x, 0] = cell.kore\n",
    "\n",
    "            shipyard = cell.shipyard\n",
    "            if shipyard:\n",
    "                # feature 1: #ships owned (shipyard)\n",
    "                state_2D[point.y, point.x, 1] = (\n",
    "                    shipyard.ship_count \n",
    "                    if shipyard.player_id == my_id \n",
    "                    else -shipyard.ship_count\n",
    "                )\n",
    "                \n",
    "                # feature 2: max spawn (shipyard)\n",
    "                state_2D[point.y, point.x, 2] = shipyard.max_spawn\n",
    "\n",
    "            fleet = cell.fleet\n",
    "            if fleet:\n",
    "                # feat 3: #ships (fleet)\n",
    "                state_2D[point.y, point.x, 3] = (\n",
    "                    fleet.ship_count\n",
    "                    if fleet.player_id == my_id\n",
    "                    else -fleet.ship_count\n",
    "                )\n",
    "                \n",
    "                # feat 4: current direction (fleet)\n",
    "                state_2D[point.y, point.x, 4] = fleet.direction.value\n",
    "\n",
    "                # feat 5: flight plan (fleet)\n",
    "                # state_2D[point.y, point.x, N_2D_FEATURES:] = self._flight_plan_str_to_arr(fleet.flight_plan)\n",
    "                state_2D[point.y, point.x, N_2D_FEATURES:] = FlightPlan().str_to_arr(fleet.flight_plan)\n",
    "\n",
    "        # For better performance, bound all features in the range [-1, 1]\n",
    "        # and as close to a normal distribution as possible\n",
    "\n",
    "        # feat 0: Logarithmic scale, kore in range [0, MAX_OBSERVABLE_KORE]\n",
    "        state_2D[:, :, 0] = clip_normalize(\n",
    "            x=np.log2(state_2D[:, :, 0] + 1),\n",
    "            low_in=0,\n",
    "            high_in=np.log2(MAX_OBSERVABLE_KORE)\n",
    "        )\n",
    "\n",
    "        # feat 1: Ships in range [-MAX_OBSERVABLE_SHIPS, MAX_OBSERVABLE_SHIPS]\n",
    "        state_2D[:, :, 1] = clip_normalize(\n",
    "            x=state_2D[:, :, 1],\n",
    "            low_in=-MAX_OBSERVABLE_SHIPS,\n",
    "            high_in=MAX_OBSERVABLE_SHIPS,\n",
    "        )\n",
    "\n",
    "        # feat 2: spawn maximum in range [1, 10]\n",
    "        state_2D[:, :, 2] = clip_normalize(\n",
    "            x=state_2D[:, :, 2],\n",
    "            low_in=MIN_SPAWN_LIMIT,\n",
    "            high_in=MAX_SPAWN_LIMIT,\n",
    "        )\n",
    "\n",
    "        # feat 3: Ships in range [-MAX_OBSERVABLE_SHIPS, MAX_OBSERVABLE_SHIPS]\n",
    "        state_2D[:, :, 3] = clip_normalize(\n",
    "            x=state_2D[:, :, 3],\n",
    "            low_in=-MAX_OBSERVABLE_SHIPS,\n",
    "            high_in=MAX_OBSERVABLE_SHIPS,\n",
    "        )\n",
    "\n",
    "        # feat 4: Fleet direction in range [1, 4]\n",
    "        state_2D[:, :, 4] = clip_normalize(\n",
    "            x=state_2D[:, :, 4],\n",
    "            low_in=1,\n",
    "            high_in=4\n",
    "        )\n",
    "\n",
    "        # feat 5: flight plan token in range [0, 5]\n",
    "        state_2D[:, :, N_2D_FEATURES:] = clip_normalize(\n",
    "            x=state_2D[:, :, N_2D_FEATURES:],\n",
    "            low_in=0,\n",
    "            high_in=5\n",
    "        )\n",
    "\n",
    "        # Flatten the input (recommended by stable_baselines3.common.env_checker.check_env)\n",
    "        output_state = state_2D.flatten()\n",
    "\n",
    "        # 1D Features\n",
    "        player = board.current_player\n",
    "        opponent = board.opponents[0]\n",
    "        progress = clip_normalize(board.step, low_in=0, high_in=GAME_CONFIG['episodeSteps'])\n",
    "        my_kore = clip_normalize(np.log2(player.kore+1), low_in=0, high_in=np.log2(MAX_KORE_IN_RESERVE))\n",
    "        opponent_kore = clip_normalize(np.log2(opponent.kore+1), low_in=0, high_in=np.log2(MAX_KORE_IN_RESERVE))\n",
    "\n",
    "        return np.append(output_state, [progress, my_kore, opponent_kore])\n",
    "\n",
    "    def compute_reward(self, done: bool, strict=False) -> float:\n",
    "        \"\"\"Compute the agent reward. Welcome to the fine art of RL.\n",
    "\n",
    "        We'll compute the reward as the current board value and a final bonus if the episode is over. If the player\n",
    "        wins the episode, we'll add a final bonus that increases with shorter time-to-victory.\n",
    "        If the player loses, we'll subtract that bonus.\n",
    "\n",
    "        Args:\n",
    "            done: True if the episode is over\n",
    "            strict: If True, count only wins/loses (Useful for evaluating a trained agent)\n",
    "\n",
    "        Returns:\n",
    "            The agent's reward\n",
    "        \"\"\"\n",
    "        board = self.board\n",
    "        previous_board = self.previous_board\n",
    "\n",
    "        if strict:\n",
    "            if done:\n",
    "                # Who won?\n",
    "                # Ugly but 99% sure correct, see https://www.kaggle.com/competitions/kore-2022/discussion/324150#1789804\n",
    "                agent_reward = self.raw_obs.players[0][0]\n",
    "                opponent_reward = self.raw_obs.players[1][0]\n",
    "                return int(agent_reward > opponent_reward)\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            if done:\n",
    "                # Who won?\n",
    "                agent_reward = self.raw_obs.players[0][0]\n",
    "                opponent_reward = self.raw_obs.players[1][0]\n",
    "                if agent_reward is None or opponent_reward is None:\n",
    "                    we_won = -1\n",
    "                else:\n",
    "                    we_won = 1 if agent_reward > opponent_reward else -1\n",
    "                win_reward = we_won * (WIN_REWARD + 5 * (GAME_CONFIG['episodeSteps'] - board.step))\n",
    "            else:\n",
    "                win_reward = 0\n",
    "\n",
    "            return get_board_value(board) - get_board_value(previous_board) + win_reward\n",
    "\n",
    "\n",
    "def clip_normalize(x: Union[np.ndarray, float],\n",
    "                   low_in: float,\n",
    "                   high_in: float,\n",
    "                   low_out=-1.,\n",
    "                   high_out=1.) -> Union[np.ndarray, float]:\n",
    "    \"\"\"Clip values in x to the interval [low_in, high_in] and then MinMax-normalize to [low_out, high_out].\n",
    "\n",
    "    Args:\n",
    "        x: The array of float to clip and normalize\n",
    "        low_in: The lowest possible value in x\n",
    "        high_in: The highest possible value in x\n",
    "        low_out: The lowest possible value in the output\n",
    "        high_out: The highest possible value in the output\n",
    "\n",
    "    Returns:\n",
    "        The clipped and normalized version of x\n",
    "\n",
    "    Raises:\n",
    "        AssertionError if the limits are not consistent\n",
    "\n",
    "    Examples:\n",
    "        >>> clip_normalize(50, low_in=0, high_in=100)\n",
    "        0.0\n",
    "\n",
    "        >>> clip_normalize(np.array([-1, .5, 99]), low_in=-1, high_in=1, low_out=0, high_out=2)\n",
    "        array([0., 1.5, 2.])\n",
    "    \"\"\"\n",
    "    assert high_in > low_in and high_out > low_out, \"Wrong limits\"\n",
    "\n",
    "    # Clip outliers\n",
    "    try:\n",
    "        x[x > high_in] = high_in\n",
    "        x[x < low_in] = low_in\n",
    "    except TypeError:\n",
    "        x = high_in if x > high_in else x\n",
    "        x = low_in if x < low_in else x\n",
    "\n",
    "    # y = ax + b\n",
    "    a = (high_out - low_out) / (high_in - low_in)\n",
    "    b = high_out - high_in * a\n",
    "\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab88bc9",
   "metadata": {
    "papermill": {
     "duration": 0.050543,
     "end_time": "2022-06-04T05:02:00.290785",
     "exception": false,
     "start_time": "2022-06-04T05:02:00.240242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Check that we have a valid environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57553a37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:02:00.394474Z",
     "iopub.status.busy": "2022-06-04T05:02:00.394195Z",
     "iopub.status.idle": "2022-06-04T05:02:00.398113Z",
     "shell.execute_reply": "2022-06-04T05:02:00.397287Z"
    },
    "papermill": {
     "duration": 0.058033,
     "end_time": "2022-06-04T05:02:00.400049",
     "exception": false,
     "start_time": "2022-06-04T05:02:00.342016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The bad news: this check will fail in the kaggle docker environment. The most likely reason is a version mismatch between packages.\n",
    "# The good news: That's alright since everything else works! We're doing some unconventional dependency management here, so we'll have to live with\n",
    "# a failed check.\n",
    "\n",
    "#from stable_baselines3.common.env_checker import check_env\n",
    "#from environment import KoreGymEnv\n",
    "\n",
    "#env = KoreGymEnv()\n",
    "#check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25355778",
   "metadata": {
    "papermill": {
     "duration": 0.05053,
     "end_time": "2022-06-04T05:02:00.501475",
     "exception": false,
     "start_time": "2022-06-04T05:02:00.450945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9465f812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:02:00.606998Z",
     "iopub.status.busy": "2022-06-04T05:02:00.606196Z",
     "iopub.status.idle": "2022-06-04T05:02:03.345943Z",
     "shell.execute_reply": "2022-06-04T05:02:03.344887Z"
    },
    "papermill": {
     "duration": 2.794629,
     "end_time": "2022-06-04T05:02:03.348507",
     "exception": false,
     "start_time": "2022-06-04T05:02:00.553878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from environment import KoreGymEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc39765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:02:03.454832Z",
     "iopub.status.busy": "2022-06-04T05:02:03.454527Z",
     "iopub.status.idle": "2022-06-04T05:02:06.934786Z",
     "shell.execute_reply": "2022-06-04T05:02:06.933925Z"
    },
    "papermill": {
     "duration": 3.53367,
     "end_time": "2022-06-04T05:02:06.937365",
     "exception": false,
     "start_time": "2022-06-04T05:02:03.403695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "kore_env = KoreGymEnv(config=dict(randomSeed=997269658))  # TODO: This seed is not enough. Seed everything!\n",
    "monitored_env = Monitor(env=kore_env)\n",
    "model = PPO('MlpPolicy', monitored_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e71aab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T05:02:07.040321Z",
     "iopub.status.busy": "2022-06-04T05:02:07.039394Z",
     "iopub.status.idle": "2022-06-04T06:23:34.590012Z",
     "shell.execute_reply": "2022-06-04T06:23:34.589041Z"
    },
    "papermill": {
     "duration": 4887.606138,
     "end_time": "2022-06-04T06:23:34.594364",
     "exception": false,
     "start_time": "2022-06-04T05:02:06.988226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 381       |\n",
      "|    ep_rew_mean     | -4.66e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 11        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 175       |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 380          |\n",
      "|    ep_rew_mean          | -5.18e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 384          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031256182 |\n",
      "|    clip_fraction        | 0.0042       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.68        |\n",
      "|    explained_variance   | 5.84e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.12e+06     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.1e+06      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 383          |\n",
      "|    ep_rew_mean          | -5.31e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 575          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046763704 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.67        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.91e+06     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 1.22e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 382          |\n",
      "|    ep_rew_mean          | -5.05e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 745          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040456667 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.68        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.62e+06     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.03e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 386         |\n",
      "|    ep_rew_mean          | -4.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 915         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005162169 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.67       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.49e+06    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 5.19e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 386          |\n",
      "|    ep_rew_mean          | -4.81e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 11           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 1085         |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063763256 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.67        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.82e+06     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00561     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 7.99e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 388          |\n",
      "|    ep_rew_mean          | -4.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 11           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 1276         |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035945624 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.66        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.49e+06     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 6.39e+06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 386         |\n",
      "|    ep_rew_mean          | -5.03e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1462        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006316223 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.66       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.8e+06     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 1.53e+07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 387         |\n",
      "|    ep_rew_mean          | -5.08e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1657        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003296118 |\n",
      "|    clip_fraction        | 0.00586     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.66       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.07e+06    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.000822   |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 8.24e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 386         |\n",
      "|    ep_rew_mean          | -5.11e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1857        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005089822 |\n",
      "|    clip_fraction        | 0.0345      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.65       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.17e+06    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 1.08e+07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 387         |\n",
      "|    ep_rew_mean          | -5.27e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 2076        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004324908 |\n",
      "|    clip_fraction        | 0.0183      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.49e+06    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00284    |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 1.07e+07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 386          |\n",
      "|    ep_rew_mean          | -5.38e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 2298         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038805625 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.63e+06     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 1.72e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 386          |\n",
      "|    ep_rew_mean          | -5.51e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 2523         |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039633773 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04e+07     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 1.69e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 387         |\n",
      "|    ep_rew_mean          | -5.54e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 2730        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004212122 |\n",
      "|    clip_fraction        | 0.0149      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.37e+06    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 1.85e+07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 387          |\n",
      "|    ep_rew_mean          | -5.51e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 2915         |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046468107 |\n",
      "|    clip_fraction        | 0.0159       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.65        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.2e+06      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 1.33e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 387         |\n",
      "|    ep_rew_mean          | -5.54e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 3123        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005628098 |\n",
      "|    clip_fraction        | 0.0385      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.65       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.11e+06    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 8.95e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 387          |\n",
      "|    ep_rew_mean          | -5.54e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 3313         |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049120584 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.65        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.24e+06     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 1.16e+07     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 388       |\n",
      "|    ep_rew_mean          | -5.55e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 10        |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 3520      |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0053195 |\n",
      "|    clip_fraction        | 0.033     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.65     |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.1e+06   |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.00378  |\n",
      "|    std                  | 0.991     |\n",
      "|    value_loss           | 1.16e+07  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 389          |\n",
      "|    ep_rew_mean          | -5.64e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 3746         |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041642366 |\n",
      "|    clip_fraction        | 0.0134       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.64        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.52e+06     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 1.3e+07      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 389          |\n",
      "|    ep_rew_mean          | -5.75e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 3945         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042643305 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.64        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.91e+06     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 1.78e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 389         |\n",
      "|    ep_rew_mean          | -5.78e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 4152        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004582959 |\n",
      "|    clip_fraction        | 0.0182      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.63       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.96e+06    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 1.62e+07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 390          |\n",
      "|    ep_rew_mean          | -5.79e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 4356         |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039175353 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.97e+06     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 1.51e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 390          |\n",
      "|    ep_rew_mean          | -5.81e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 4534         |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043045883 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.82e+06     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 1.08e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 390          |\n",
      "|    ep_rew_mean          | -5.82e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 4692         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065412642 |\n",
      "|    clip_fraction        | 0.0432       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.62        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.38e+06     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 9.84e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 389          |\n",
      "|    ep_rew_mean          | -5.9e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 4885         |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041906275 |\n",
      "|    clip_fraction        | 0.016        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.61        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.91e+06     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 7.76e+06     |\n",
      "------------------------------------------\n",
      "CPU times: user 1h 20min 57s, sys: 26.3 s, total: 1h 21min 24s\n",
      "Wall time: 1h 21min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f24d583e490>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# For serious training, likely many more iterations will be needed, as well as hyperparameter tuning!\n",
    "# Even so, sometimes training will still fail. RL is like that. Try a couple times with the same config before giving up!\n",
    "model.learn(total_timesteps=50000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaeb2b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T06:23:34.720036Z",
     "iopub.status.busy": "2022-06-04T06:23:34.719758Z",
     "iopub.status.idle": "2022-06-04T06:23:34.723243Z",
     "shell.execute_reply": "2022-06-04T06:23:34.722485Z"
    },
    "papermill": {
     "duration": 0.070497,
     "end_time": "2022-06-04T06:23:34.725123",
     "exception": false,
     "start_time": "2022-06-04T06:23:34.654626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Note: The current episode might not be over yet\n",
    "# kore_env.render(mode=\"ipython\", width=1000, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a5f6f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T06:23:34.845084Z",
     "iopub.status.busy": "2022-06-04T06:23:34.844814Z",
     "iopub.status.idle": "2022-06-04T06:23:34.897428Z",
     "shell.execute_reply": "2022-06-04T06:23:34.896655Z"
    },
    "papermill": {
     "duration": 0.115828,
     "end_time": "2022-06-04T06:23:34.899761",
     "exception": false,
     "start_time": "2022-06-04T06:23:34.783933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"baseline_agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f8b769",
   "metadata": {
    "papermill": {
     "duration": 0.059418,
     "end_time": "2022-06-04T06:23:35.019885",
     "exception": false,
     "start_time": "2022-06-04T06:23:34.960467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate agent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea06510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T06:23:35.141839Z",
     "iopub.status.busy": "2022-06-04T06:23:35.141569Z",
     "iopub.status.idle": "2022-06-04T06:23:35.146565Z",
     "shell.execute_reply": "2022-06-04T06:23:35.145792Z"
    },
    "papermill": {
     "duration": 0.067387,
     "end_time": "2022-06-04T06:23:35.148354",
     "exception": false,
     "start_time": "2022-06-04T06:23:35.080967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# eval_env = KoreGymEnv(config=dict(strict=True))  # The 'strict' flags sets rewards to 1 if the agent won the episode and 0 else. Useful for evaluation.\n",
    "# monitored_env = Monitor(env=eval_env)\n",
    "# model_loaded = PPO.load('baseline_agent')\n",
    "\n",
    "# def evaluate(model, num_episodes=1):\n",
    "#     \"\"\"\n",
    "#     Evaluate a RL agent - Adapted from \n",
    "#     https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb\n",
    "#     :param model: (BaseRLModel object) the RL Agent\n",
    "#     :param num_episodes: (int) number of episodes to evaluate it\n",
    "#     :return: (float) Mean reward for the last num_episodes\n",
    "#     \"\"\"\n",
    "#     all_episode_rewards = []\n",
    "#     for i in range(num_episodes):\n",
    "#         episode_rewards = []\n",
    "#         done = False\n",
    "#         obs = monitored_env.reset()\n",
    "#         while not done:\n",
    "#             action, _ = model.predict(obs)\n",
    "#             obs, _, done, info = monitored_env.step(action)\n",
    "#             if done:\n",
    "#                 agent_reward = monitored_env.env.raw_obs.players[0][0]\n",
    "#                 opponent_reward = monitored_env.env.raw_obs.players[1][0]\n",
    "#                 reward = agent_reward > opponent_reward\n",
    "#             else:\n",
    "#                 reward = 0\n",
    "#             # print(reward)\n",
    "#             # monitored_env.render(mode='ipython', height=400, width=300)\n",
    "#             episode_rewards.append(reward)\n",
    "\n",
    "#         all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "#     mean_episode_reward = np.mean(all_episode_rewards)\n",
    "#     print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "#     return mean_episode_reward\n",
    "\n",
    "# evaluate(model_loaded, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f507d1",
   "metadata": {
    "papermill": {
     "duration": 0.060083,
     "end_time": "2022-06-04T06:23:35.267785",
     "exception": false,
     "start_time": "2022-06-04T06:23:35.207702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c8d072b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T06:23:35.391830Z",
     "iopub.status.busy": "2022-06-04T06:23:35.391085Z",
     "iopub.status.idle": "2022-06-04T06:23:35.397253Z",
     "shell.execute_reply": "2022-06-04T06:23:35.396440Z"
    },
    "papermill": {
     "duration": 0.070591,
     "end_time": "2022-06-04T06:23:35.399847",
     "exception": false,
     "start_time": "2022-06-04T06:23:35.329256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "# All this syspath wranglig is needed to make sure that the agent runs on the target environment and can load both the external dependencies\n",
    "# and the saved model. Dear kaggle, if possible, please make this easier!\n",
    "import os\n",
    "import sys\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # We're in the kaggle target system\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    agent_path = os.path.join(KAGGLE_AGENT_PATH, 'baseline_agent')\n",
    "else:\n",
    "    # We're somewhere else\n",
    "    sys.path.insert(0, os.path.join(os.getcwd(), 'lib'))\n",
    "    agent_path = 'baseline_agent'\n",
    "\n",
    "\n",
    "# Now for the actual agent\n",
    "from stable_baselines3 import PPO\n",
    "from environment import KoreGymEnv\n",
    "\n",
    "model = PPO.load(agent_path)\n",
    "kore_env = KoreGymEnv()\n",
    "\n",
    "def agent(obs, config):\n",
    "    kore_env.raw_obs = obs\n",
    "    state = kore_env.obs_as_gym_state\n",
    "    action, _ = model.predict(state)\n",
    "    return kore_env.gym_to_kore_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51755f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T06:23:35.527310Z",
     "iopub.status.busy": "2022-06-04T06:23:35.526512Z",
     "iopub.status.idle": "2022-06-04T06:23:35.530504Z",
     "shell.execute_reply": "2022-06-04T06:23:35.529630Z"
    },
    "papermill": {
     "duration": 0.0679,
     "end_time": "2022-06-04T06:23:35.532408",
     "exception": false,
     "start_time": "2022-06-04T06:23:35.464508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # This is for debugging purposes only before submitting - Are there any errors?\n",
    "# from kaggle_environments import make\n",
    "# from config import OPPONENT\n",
    "# env = make(\"kore_fleets\", debug=True)\n",
    "# env.run(['main.py', OPPONENT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea459665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T06:23:35.654777Z",
     "iopub.status.busy": "2022-06-04T06:23:35.654502Z",
     "iopub.status.idle": "2022-06-04T06:23:37.440327Z",
     "shell.execute_reply": "2022-06-04T06:23:37.438988Z"
    },
    "papermill": {
     "duration": 1.850503,
     "end_time": "2022-06-04T06:23:37.443021",
     "exception": false,
     "start_time": "2022-06-04T06:23:35.592518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -czf submission.tar.gz main.py config.py environment.py reward_utils.py helper.py baseline_agent.zip lib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4942.718994,
   "end_time": "2022-06-04T06:23:40.436335",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-04T05:01:17.717341",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
